{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "kusto-bg3"
		},
		"AzureDataLakeStorage1_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1'"
		},
		"AzureDataLakeStorage1116_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1116'"
		},
		"AzureDataLakeStorage1117_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1117'"
		},
		"BlobStorage112502_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'BlobStorage112502'"
		},
		"BlobStorage121701_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'BlobStorage121701'"
		},
		"BlobStorage121702_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'BlobStorage121702'"
		},
		"Gen2112501_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'Gen2112501'"
		},
		"Gen21217_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'Gen21217'"
		},
		"Kusto112501_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'Kusto112501'"
		},
		"MongoDbApi1125_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'MongoDbApi1125'"
		},
		"aadsharma-github-cicd-1-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'aadsharma-github-cicd-1-WorkspaceDefaultSqlServer'"
		},
		"gen21122_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'gen21122'"
		},
		"github-1-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'github-1-WorkspaceDefaultSqlServer'"
		},
		"github-3-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'github-3-WorkspaceDefaultSqlServer'"
		},
		"kusto-bg3-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'kusto-bg3-WorkspaceDefaultSqlServer'"
		},
		"kusto1122_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'kusto1122'"
		},
		"kusto1123_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'kusto1123'"
		},
		"nyc_tlc_green_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'nyc_tlc_green'"
		},
		"nyc_tlc_green_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'nyc_tlc_green'"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaotestgen2.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1116_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://storagegen2qingtest.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1117_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://storagegen2qingtest.dfs.core.windows.net"
		},
		"AzureKeyVault1_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://a365-e2e.vault.azure.net/"
		},
		"Gen1112501_properties_typeProperties_dataLakeStoreUri": {
			"type": "string",
			"defaultValue": "https://storagegen1qingtest.azuredatalakestore.net/webhdfs/v1"
		},
		"Gen1112501_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"Gen1112501_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3"
		},
		"Gen1112501_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "bigdataqa"
		},
		"Gen2112501_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://storagegen2qingtest.dfs.core.windows.net"
		},
		"Gen21217_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://storagegen2qingtest.dfs.core.windows.net"
		},
		"Kusto112501_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"Kusto112501_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "ac3a0e33-1db1-48ba-9b19-24ce0330ff5c"
		},
		"Kusto112501_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "cdndemo"
		},
		"MongoDbApi1125_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "yuwwang-mongo-db1"
		},
		"aadsharma-github-cicd-1-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://aadsharmadl.dfs.core.windows.net"
		},
		"gen11123_properties_typeProperties_dataLakeStoreUri": {
			"type": "string",
			"defaultValue": "https://ruxuadlsgen1.azuredatalakestore.net/webhdfs/v1"
		},
		"gen11123_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"gen11123_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3"
		},
		"gen11123_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "ruxurg"
		},
		"gen21122_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://storagegen2qingtest.dfs.core.windows.net"
		},
		"github-1-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://actionbugbash.dfs.core.windows.net"
		},
		"github-3-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://actionbugbash.dfs.core.windows.net"
		},
		"kusto-bg3-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://kustobg.dfs.core.windows.net"
		},
		"kusto1122_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"kusto1122_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "123"
		},
		"kusto1122_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "789"
		},
		"kusto1123_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"kusto1123_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "34"
		},
		"kusto1123_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "87654"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook1",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Notebook 1",
								"type": "NotebookReference"
							},
							"snapshot": true
						}
					}
				],
				"annotations": [],
				"lastPublishTime": "2021-05-27T12:46:11Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Notebook 1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1_copy1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook1",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Notebook 1",
								"type": "NotebookReference"
							},
							"snapshot": true
						}
					}
				],
				"annotations": [],
				"lastPublishTime": "2021-05-27T12:46:11Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Notebook 1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1_copy2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook1",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Notebook 1",
								"type": "NotebookReference"
							},
							"snapshot": true
						}
					}
				],
				"annotations": [],
				"lastPublishTime": "2021-05-27T12:46:10Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Notebook 1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1_copy3')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook1",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Notebook 1",
								"type": "NotebookReference"
							},
							"snapshot": true
						}
					}
				],
				"annotations": [],
				"lastPublishTime": "2021-05-27T12:46:13Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Notebook 1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "displaynotebook",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "displaynotebook",
								"type": "NotebookReference"
							},
							"snapshot": true
						}
					}
				],
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/displaynotebook')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1116')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1116_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1116_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1117')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1117_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1117_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureKeyVault1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('AzureKeyVault1_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/BlobStorage112502')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "01",
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('BlobStorage112502_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/BlobStorage121701')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('BlobStorage121701_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/BlobStorage121702')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('BlobStorage121702_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Gen1112501')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataLakeStore",
				"typeProperties": {
					"dataLakeStoreUri": "[parameters('Gen1112501_properties_typeProperties_dataLakeStoreUri')]",
					"tenant": "[parameters('Gen1112501_properties_typeProperties_tenant')]",
					"subscriptionId": "[parameters('Gen1112501_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('Gen1112501_properties_typeProperties_resourceGroupName')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Gen2112501')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "1204",
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('Gen2112501_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('Gen2112501_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Gen21217')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('Gen21217_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('Gen21217_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Kusto112501')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "1204",
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "https://kustodemo.westus2.kusto.windows.net",
					"tenant": "[parameters('Kusto112501_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('Kusto112501_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('Kusto112501_servicePrincipalKey')]"
					},
					"database": "[parameters('Kusto112501_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MongoDbApi1125')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "01",
				"annotations": [],
				"type": "CosmosDbMongoDbApi",
				"typeProperties": {
					"connectionString": "[parameters('MongoDbApi1125_connectionString')]",
					"database": "[parameters('MongoDbApi1125_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PowerBIWorkspace1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "PowerBIWorkspace",
				"typeProperties": {
					"workspaceID": "28a0e6b5-f83b-47c9-aa9f-eaab6b8a2bf9",
					"tenantID": "72f988bf-86f1-41af-91ab-2d7cd011db47"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/aadsharma-github-cicd-1-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('aadsharma-github-cicd-1-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/aadsharma-github-cicd-1-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('aadsharma-github-cicd-1-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/gen11123')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataLakeStore",
				"typeProperties": {
					"dataLakeStoreUri": "[parameters('gen11123_properties_typeProperties_dataLakeStoreUri')]",
					"tenant": "[parameters('gen11123_properties_typeProperties_tenant')]",
					"subscriptionId": "[parameters('gen11123_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('gen11123_properties_typeProperties_resourceGroupName')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/gen21122')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "test",
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('gen21122_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('gen21122_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/github-1-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('github-1-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/github-1-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('github-1-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/github-3-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('github-3-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/github-3-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('github-3-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kusto-bg3-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('kusto-bg3-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kusto-bg3-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('kusto-bg3-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kusto1122')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "test1",
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "https://lirsuntest.southeastasia.kusto.windows.net",
					"tenant": "[parameters('kusto1122_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('kusto1122_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('kusto1122_servicePrincipalKey')]"
					},
					"database": "[parameters('kusto1122_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kusto1123')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "https://lirsuntest.southeastasia.kusto.windows.net",
					"tenant": "[parameters('kusto1123_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('kusto1123_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('kusto1123_servicePrincipalKey')]"
					},
					"database": "[parameters('kusto1123_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nyc_tlc_green')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('nyc_tlc_green_connectionString')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('nyc_tlc_green_accountKey')]"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 1')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Pipeline 1",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 15,
						"startTime": "2021-09-02T14:22:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Pipeline 1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 2')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Pipeline 2",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 15,
						"startTime": "2021-09-02T14:22:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Pipeline 2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 3')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Pipeline 2",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 15,
						"startTime": "2021-09-02T14:29:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Pipeline 2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 4')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Pipeline 2",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 15,
						"startTime": "2021-09-02T14:29:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Pipeline 2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime1')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0,
							"cleanup": true
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime1125')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "South Central US",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MYSQLIR')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow1')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "github-1-WorkspaceDefaultStorage",
								"type": "LinkedServiceReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						}
					],
					"transformations": [],
					"script": "source(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false,\n\tformat: 'excel',\n\tfileSystem: 'workspace',\n\tfileName: 'Accessibility All.csv',\n\tsheetName: 'sheet1',\n\tfirstRowAsHeader: false) ~> source1\nsource1 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true,\n\tstore: 'cache',\n\tformat: 'inline',\n\toutput: false,\n\tsaveOrder: 1) ~> sink1"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/github-1-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Query CSV files')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/* Covid-19 ECDC cases opendata set */\n\n/* Read a csv file */\nselect top 10 *\nfrom openrowset(\n    bulk 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.csv',\n    format = 'csv',\n    parser_version = '2.0',\n    firstrow = 2 ) as rows\n\n\n\n/* Explicitly specify schema */\nselect top 10 *\nfrom openrowset(\n        bulk 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.csv',\n        format = 'csv',\n        parser_version ='2.0',\n        firstrow = 2\n    ) with (\n        date_rep date 1,\n        cases int 5,\n        geo_id varchar(6) 8\n    ) as rows\n\n/* Windows style new line */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '\n'\n    )\nWITH (\n    [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n    [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n    [year] smallint,\n    [population] bigint\n) AS [r]\nWHERE\n    country_name = 'Luxembourg'\n    AND year = 2017;\n\n\n/* Unix-style new line */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population-unix/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '0x0a'\n    )\nWITH (\n    [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n    [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n    [year] smallint,\n    [population] bigint\n) AS [r]\nWHERE\n    country_name = 'Luxembourg'\n    AND year = 2017;\n\n\n/* Header row */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population-unix-hdr/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        FIRSTROW = 2\n    )\n    WITH (\n        [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n        [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n        [year] smallint,\n        [population] bigint\n    ) AS [r]\nWHERE\n    country_name = 'Luxembourg'\n    AND year = 2017;\n\n\n/* Custom quote character */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population-unix-hdr-quoted/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '0x0a',\n        FIRSTROW = 2,\n        FIELDQUOTE = '\"'\n    )\n    WITH (\n        [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n        [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n        [year] smallint,\n        [population] bigint\n    ) AS [r]\nWHERE\n    country_name = 'Luxembourg'\n    AND year = 2017;\n\n\n/* Escape characters */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population-unix-hdr-escape/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '0x0a',\n        FIRSTROW = 2,\n        ESCAPECHAR = ''\n    )\n    WITH (\n        [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n        [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n        [year] smallint,\n        [population] bigint\n    ) AS [r]\nWHERE\n    country_name = 'Slovenia';\n\n\n/* Escape quoting characters */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population-unix-hdr-escape-quoted/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '0x0a',\n        FIRSTROW = 2\n    )\n    WITH (\n        [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n        [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n        [year] smallint,\n        [population] bigint\n    ) AS [r]\nWHERE\n    country_name = 'Slovenia';\n\n\n/* Tab-delimited files */\nSELECT *\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population-unix-hdr-tsv/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR ='\t',\n        ROWTERMINATOR = '0x0a',\n        FIRSTROW = 2\n    )\n    WITH (\n        [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n        [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n        [year] smallint,\n        [population] bigint\n    ) AS [r]\nWHERE\n    country_name = 'Luxembourg'\n    AND year = 2017\n\n\n/* Return a subset of columns */\nSELECT\n    COUNT(DISTINCT country_name) AS countries\nFROM OPENROWSET(\n        BULK 'https://sqlondemandstorage.blob.core.windows.net/public-csv/population/population.csv',\n        FORMAT = 'CSV', PARSER_VERSION = '2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '\n'\n    )\nWITH (\n    --[country_code] VARCHAR (5),\n    [country_name] VARCHAR (100) 2\n    --[year] smallint,\n    --[population] bigint\n) AS [r]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1 - git')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "select * from dummy;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sql test 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getting Started with Delta Lake')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (Python)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000000)\n",
							"delta_table_path = \"/delta/delta-table-{0}\".format(session_id)\n",
							"\n",
							"delta_table_path"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(0,5)\n",
							"data.show()\n",
							"data.write.format(\"delta\").save(delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"[[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").load(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(5,10)\n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"[[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Write data to a new managed catalog table.\n",
							"data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.sql(\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{0}'\".format(delta_table_path))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# List the 2 new tables.\n",
							"spark.sql(\"SHOW TABLES\").show()\n",
							"\n",
							"# Explore their properties.\n",
							"spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").show(truncate=False)\n",
							"spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").show(truncate=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"delta_table = DeltaTable.forPath(spark, delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Update every even value by adding 100 to it\n",
							"delta_table.update(\n",
							"  condition = expr(\"id % 2 == 0\"),\n",
							"  set = { \"id\": expr(\"id + 100\") })\n",
							"delta_table.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delete every even value\n",
							"delta_table.delete(\"id % 2 == 0\")\n",
							"delta_table.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Upsert (merge) new data\n",
							"new_data = spark.range(0,20).alias(\"newData\")\n",
							"\n",
							"delta_table.alias(\"oldData\")\\\n",
							"    .merge(new_data.alias(\"newData\"), \"oldData.id = newData.id\")\\\n",
							"    .whenMatchedUpdate(set = { \"id\": lit(\"-1\")})\\\n",
							"    .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") })\\\n",
							"    .execute()\n",
							"\n",
							"delta_table.toDF().show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().show(20, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"streaming_df = spark.readStream.format(\"rate\").load()\n",
							"stream = streaming_df\\\n",
							"    .selectExpr(\"value as id\")\\\n",
							"    .writeStream\\\n",
							"    .format(\"delta\")\\\n",
							"    .option(\"checkpointLocation\", \"/tmp/checkpoint-{0}\".format(session_id))\\\n",
							"    .start(delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.toDF().sort(col(\"id\").desc()).show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"stream.stop()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(100, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = \"/parquet/parquet-table-{0}\".format(session_id)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.convertToDelta(spark, \"parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"# Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DESCRIBE HISTORY delta.`{0}`\".format(delta_table_path)).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"VACUUM delta.`{0}`\".format(delta_table_path)).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_id = random.randint(0,1000)\n",
							"parquet_path = \"/parquet/parquet-table-{0}-{1}\".format(session_id, parquet_path)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)\n",
							"\n",
							"# Use SQL to convert the parquet table to Delta\n",
							"spark.sql(\"CONVERT TO DELTA parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/InputOutputTest')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"val rdd = sc.textFile(\"abfss://mydefault@ltianscusgen2.dfs.core.windows.net/xiaolel/test1/part-00004-c914640a-2e14-4687-9f5e-457ba4ebff8f-c000.csv\")\n",
							"rdd.count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"rdd.saveAsTextFile(\"/test/output\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"val another = spark.read.csv(\"abfss://mydefault@ltianscusgen2.dfs.core.windows.net/xiaolel/test1/part-00003-c914640a-2e14-4687-9f5e-457ba4ebff8f-c000.csv\")\n",
							"another.count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"another.write.csv(\"/dataframeoutput/test1\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/InputOutputTest_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"val rdd = sc.textFile(\"abfss://mydefault@ltianscusgen2.dfs.core.windows.net/xiaolel/test1/part-00004-c914640a-2e14-4687-9f5e-457ba4ebff8f-c000.csv\")\n",
							"rdd.count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"rdd.saveAsTextFile(\"/test/output\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"val another = spark.read.csv(\"abfss://mydefault@ltianscusgen2.dfs.core.windows.net/xiaolel/test1/part-00003-c914640a-2e14-4687-9f5e-457ba4ebff8f-c000.csv\")\n",
							"another.count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"another.write.csv(\"/dataframeoutput/test1\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1 - git')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpoolgit",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/fcf65c12-e569-4fe5-8433-b4142d1f6219/resourceGroups/aadsharmarg/providers/Microsoft.Synapse/workspaces/aadsharma-github-cicd-1/bigDataPools/sparkpoolgit",
						"name": "sparkpoolgit",
						"type": "Spark",
						"endpoint": "https://aadsharma-github-cicd-1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpoolgit",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"print(\"hello\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/displayhtmlnotebook_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import displayHTML\n",
							"displayHTML(\"hello world\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/displaynotebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import display\n",
							"display.config(\"\")\n",
							"display(spark.range(1))\n",
							"display.execute(\"\") "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/matplotlibnotebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import matplotlib\n",
							"import matplotlib.pyplot as plt\n",
							"from notebookutils import enableMatplotlib\n",
							"from notebookutils.visualization.msInlinePlotlib import MsInLineBackend\n",
							"enableMatplotlib()\n",
							"if not matplotlib.__version__.startswith(\"3.3\") and (plt._show != MsInLineBackend.show):\n",
							"    plt._show = MsInLineBackend.show\n",
							"else:\n",
							"    pass\n",
							"plt.plot([1, 2, 3, 4])\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/matplotlibnotebook_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder 1"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import matplotlib\n",
							"import matplotlib.pyplot as plt\n",
							"from notebookutils import enableMatplotlib\n",
							"from notebookutils.visualization.msInlinePlotlib import MsInLineBackend\n",
							"enableMatplotlib()\n",
							"if not matplotlib.__version__.startswith(\"3.3\") and (plt._show != MsInLineBackend.show):\n",
							"    plt._show = MsInLineBackend.show\n",
							"else:\n",
							"    pass\n",
							"plt.plot([1, 2, 3, 4])\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mssparkutilsnotebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils\n",
							"mssparkutils.fs.help()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.env.help()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "csharp"
							}
						},
						"source": [
							"%%csharp\n",
							"using static Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.Visualization.Functions;\n",
							"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.MSSparkUtils;\n",
							"FS.Help()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "csharp"
							}
						},
						"source": [
							"%%csharp\n",
							"Env.Help()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/outputAnalyzer')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder 1"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## 0. Set the output location\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"//please replace the tagName in the path below for further analysis.\\nval baseOutputPath = \\\"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\\\"\\nval sparkConfPath = baseOutputPath + \\\"confJson\\\"\\nval queryMetricsPath = baseOutputPath + \\\"query-metrics/\\\"\\nval sparkEventsPath = baseOutputPath + \\\"spark-events/\\\"\\nval systemMetricsSummary = baseOutputPath + \\\"/summary/system-metrics-summary/\\\"\","
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"//please replace the tagName in the path below for further analysis.\n",
							"val baseOutputPath = \"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\"\n",
							"val sparkConfPath = baseOutputPath + \"confJson\"\n",
							"val queryMetricsPath = baseOutputPath + \"query-metrics/\"\n",
							"val sparkEventsPath = baseOutputPath + \"spark-events/\"\n",
							"val systemMetricsSummary = baseOutputPath + \"/summary/system-metrics-summary/\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 1. Spark Configurations\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkConf = spark.read.json(sparkConfPath)\\nsparkConf.select(\"spark.`spark.driver.cores`\", \"hadoop.`dfs.heartbeat.interval`\" ).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 2. queryMetrics: Get Query execution times\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Query(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    startTime: Long,\n",
							"    endTime:Long,\n",
							"    executionTimeMs: Long,\n",
							"    queryResultValidationSuccess: Boolean,\n",
							"    fileScanTimeMs: Long\n",
							"    )\n",
							"    \n",
							"    val queryMetrics = spark.read.option(\"basePath\", queryMetricsPath).json(s\"${queryMetricsPath}/*/**\")\n",
							"    val query = queryMetrics.select(\n",
							"        $\"queryId\",\n",
							"        $\"runId\",\n",
							"        $\"queryResultValidationSuccess\",\n",
							"        $\"runtimes.startTime\".alias(\"startTime\"),\n",
							"        $\"runTimes.endTime\".alias(\"endTime\"),\n",
							"        $\"runTimes.executionTimeMs\".alias(\"executionTimeMs\"),\n",
							"        $\"fileScanTimeMs\".alias(\"fileScanTimeMs\")\n",
							"        ).as[Query]\n",
							"        \n",
							"    queryMetrics.show(5)\n",
							"    query.createOrReplaceTempView(\"query\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"SELECT queryId, min(executionTimeMs), max(executionTimeMs) from query group by 1 order by 1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.0 Spark Events \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkEvents = spark.read.option(\"basePath\", sparkEventsPath).json(s\"${sparkEventsPath}/*/**\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.1 Spark Events: Task details\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Task(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    taskId: Long,\n",
							"    taskAttemptId: Long,\n",
							"    stageId: Long,\n",
							"    stageAttemptId: Long,\n",
							"    executorId: String,\n",
							"    host: String,\n",
							"    startTime: Long,\n",
							"    finishTime: Long,\n",
							"    killed: Boolean,\n",
							"    failed: Boolean,\n",
							"    shuffleBytesWritten: Long,\n",
							"    shuffleBytesRead: Long,\n",
							"    jvmGCTime: Long,\n",
							"    memoryBytesSpilled: Long,\n",
							"    diskBytesSpilled: Long,\n",
							"    taskType: String,\n",
							"    locality: String,\n",
							"    taskEndReason: String,\n",
							"    fileScanTime: Long)\n",
							"val taskEvents = sparkEvents.filter($\"Event\" === \"SparkListenerTaskEnd\").select(\n",
							"    $\"queryId\".alias(\"queryId\"),\n",
							"    $\"Task Info.Task ID\".alias(\"taskId\"),\n",
							"    $\"Task Info.Attempt\"alias(\"taskAttemptId\"),\n",
							"    $\"Stage ID\".alias(\"stageId\"),\n",
							"    $\"Stage Attempt ID\".alias(\"stageAttemptId\"),\n",
							"    $\"Task Info.Executor ID\".alias(\"executorId\"),\n",
							"    $\"Task Info.Host\".alias(\"host\"),\n",
							"    $\"Task Info.Launch Time\".alias(\"startTime\"),\n",
							"    $\"Task Info.Finish Time\".alias(\"finishTime\"),\n",
							"    $\"Task Info.Killed\".alias(\"killed\"),\n",
							"    $\"Task Info.Failed\".alias(\"failed\"),\n",
							"    $\"Task Metrics.Shuffle Write Metrics.Shuffle Bytes Written\".alias(\"shuffleBytesWritten\"),\n",
							"    ($\"Task Metrics.Shuffle Read Metrics.Remote Bytes Read\" + $\"Task Metrics.Shuffle Read Metrics.Local Bytes Read\").alias(\"shuffleBytesRead\\),\n",
							"    $\"Task Metrics.JVM GC Time\".alias(\"jvmGCTime\"),\n",
							"    $\"Task Metrics.Memory Bytes Spilled\".alias(\"memoryBytesSpilled\"),\n",
							"    $\"Task Metrics.Disk Bytes Spilled\".alias(\"diskBytesSpilled\") ,\n",
							"    $\"Task Type\".alias(\"taskType\"),\n",
							"    $\"Task Info.Locality\".alias(\"locality\"),\n",
							"    $\"Task End Reason\".alias(\"taskEndReason\"),\n",
							"    $\"Task Info.Accumulables.Value\".alias(\"accumulablesValue\"),\n",
							"    array_position($\"Task Info.Accumulables.Name\", \"scan time total (min, med, max)\").alias(\"fileScanTimeIndex\"))\n",
							"val task = taskEvents.select(\n",
							"    $\"queryId\",\n",
							"    $\"taskId\",\n",
							"    $\"taskAttemptId\",\n",
							"    $\"stageId\",\n",
							"    $\"stageAttemptId\",\n",
							"    $\"executorId\",\n",
							"    $\"host\",\n",
							"    $\"startTime\",\n",
							"    $\"finishTime\",\n",
							"    $\"killed\",\n",
							"    $\"failed\",\n",
							"    $\"shuffleBytesWritten\",\n",
							"    $\"shuffleBytesRead\",\n",
							"    $\"jvmGCTime\",\n",
							"    $\"memoryBytesSpilled\",\n",
							"    $\"diskBytesSpilled\",\n",
							"    $\"taskType\",\n",
							"    $\"locality\",\n",
							"    $\"taskEndReason\",\n",
							"    expr(\"CAST( Case when fileScanTimeIndex = 0 then 0 else element_at(accumulablesValue, CAST(fileScanTimeIndex AS INTEGER)) end AS BIGINT)\").alias(\"fileScanTime\")\n",
							"    ).as(\"t\").join(query.as(\"q\"), query(\"queryId\") === taskEvents(\"queryId\") &&  query(\"startTime\") < taskEvents(\"startTime\") && query(\"endTime\") > taskEvents(\"finishTime\")).select(\"t.*\", \"q.runId\").as[Task]\n",
							"task.createOrReplaceTempView(\"task\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"with totalTasks as\n",
							"(select queryId, runId, count(1) as totalTasks from task group by 1,2 order by 1,2),\n",
							"successfulTasks as\n",
							"(select queryId, runId, count(1) as successfulTasks from task where killed = \"false\" and failed = \"false\" group by 1,2 order by 1,2)\n",
							"select tt.queryId, tt.runId, totalTasks-successfulTasks as failedTasks, successfulTasks from totalTasks tt,successfulTasks st where st.queryId = tt.queryId and st.runId = tt.runId"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/outputAnalyzer_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder 1"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## 0. Set the output location\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"//please replace the tagName in the path below for further analysis.\\nval baseOutputPath = \\\"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\\\"\\nval sparkConfPath = baseOutputPath + \\\"confJson\\\"\\nval queryMetricsPath = baseOutputPath + \\\"query-metrics/\\\"\\nval sparkEventsPath = baseOutputPath + \\\"spark-events/\\\"\\nval systemMetricsSummary = baseOutputPath + \\\"/summary/system-metrics-summary/\\\"\","
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"//please replace the tagName in the path below for further analysis.\n",
							"val baseOutputPath = \"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\"\n",
							"val sparkConfPath = baseOutputPath + \"confJson\"\n",
							"val queryMetricsPath = baseOutputPath + \"query-metrics/\"\n",
							"val sparkEventsPath = baseOutputPath + \"spark-events/\"\n",
							"val systemMetricsSummary = baseOutputPath + \"/summary/system-metrics-summary/\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 1. Spark Configurations\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkConf = spark.read.json(sparkConfPath)\\nsparkConf.select(\"spark.`spark.driver.cores`\", \"hadoop.`dfs.heartbeat.interval`\" ).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 2. queryMetrics: Get Query execution times\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Query(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    startTime: Long,\n",
							"    endTime:Long,\n",
							"    executionTimeMs: Long,\n",
							"    queryResultValidationSuccess: Boolean,\n",
							"    fileScanTimeMs: Long\n",
							"    )\n",
							"    \n",
							"    val queryMetrics = spark.read.option(\"basePath\", queryMetricsPath).json(s\"${queryMetricsPath}/*/**\")\n",
							"    val query = queryMetrics.select(\n",
							"        $\"queryId\",\n",
							"        $\"runId\",\n",
							"        $\"queryResultValidationSuccess\",\n",
							"        $\"runtimes.startTime\".alias(\"startTime\"),\n",
							"        $\"runTimes.endTime\".alias(\"endTime\"),\n",
							"        $\"runTimes.executionTimeMs\".alias(\"executionTimeMs\"),\n",
							"        $\"fileScanTimeMs\".alias(\"fileScanTimeMs\")\n",
							"        ).as[Query]\n",
							"        \n",
							"    queryMetrics.show(5)\n",
							"    query.createOrReplaceTempView(\"query\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"SELECT queryId, min(executionTimeMs), max(executionTimeMs) from query group by 1 order by 1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.0 Spark Events \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkEvents = spark.read.option(\"basePath\", sparkEventsPath).json(s\"${sparkEventsPath}/*/**\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.1 Spark Events: Task details\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Task(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    taskId: Long,\n",
							"    taskAttemptId: Long,\n",
							"    stageId: Long,\n",
							"    stageAttemptId: Long,\n",
							"    executorId: String,\n",
							"    host: String,\n",
							"    startTime: Long,\n",
							"    finishTime: Long,\n",
							"    killed: Boolean,\n",
							"    failed: Boolean,\n",
							"    shuffleBytesWritten: Long,\n",
							"    shuffleBytesRead: Long,\n",
							"    jvmGCTime: Long,\n",
							"    memoryBytesSpilled: Long,\n",
							"    diskBytesSpilled: Long,\n",
							"    taskType: String,\n",
							"    locality: String,\n",
							"    taskEndReason: String,\n",
							"    fileScanTime: Long)\n",
							"val taskEvents = sparkEvents.filter($\"Event\" === \"SparkListenerTaskEnd\").select(\n",
							"    $\"queryId\".alias(\"queryId\"),\n",
							"    $\"Task Info.Task ID\".alias(\"taskId\"),\n",
							"    $\"Task Info.Attempt\"alias(\"taskAttemptId\"),\n",
							"    $\"Stage ID\".alias(\"stageId\"),\n",
							"    $\"Stage Attempt ID\".alias(\"stageAttemptId\"),\n",
							"    $\"Task Info.Executor ID\".alias(\"executorId\"),\n",
							"    $\"Task Info.Host\".alias(\"host\"),\n",
							"    $\"Task Info.Launch Time\".alias(\"startTime\"),\n",
							"    $\"Task Info.Finish Time\".alias(\"finishTime\"),\n",
							"    $\"Task Info.Killed\".alias(\"killed\"),\n",
							"    $\"Task Info.Failed\".alias(\"failed\"),\n",
							"    $\"Task Metrics.Shuffle Write Metrics.Shuffle Bytes Written\".alias(\"shuffleBytesWritten\"),\n",
							"    ($\"Task Metrics.Shuffle Read Metrics.Remote Bytes Read\" + $\"Task Metrics.Shuffle Read Metrics.Local Bytes Read\").alias(\"shuffleBytesRead\\),\n",
							"    $\"Task Metrics.JVM GC Time\".alias(\"jvmGCTime\"),\n",
							"    $\"Task Metrics.Memory Bytes Spilled\".alias(\"memoryBytesSpilled\"),\n",
							"    $\"Task Metrics.Disk Bytes Spilled\".alias(\"diskBytesSpilled\") ,\n",
							"    $\"Task Type\".alias(\"taskType\"),\n",
							"    $\"Task Info.Locality\".alias(\"locality\"),\n",
							"    $\"Task End Reason\".alias(\"taskEndReason\"),\n",
							"    $\"Task Info.Accumulables.Value\".alias(\"accumulablesValue\"),\n",
							"    array_position($\"Task Info.Accumulables.Name\", \"scan time total (min, med, max)\").alias(\"fileScanTimeIndex\"))\n",
							"val task = taskEvents.select(\n",
							"    $\"queryId\",\n",
							"    $\"taskId\",\n",
							"    $\"taskAttemptId\",\n",
							"    $\"stageId\",\n",
							"    $\"stageAttemptId\",\n",
							"    $\"executorId\",\n",
							"    $\"host\",\n",
							"    $\"startTime\",\n",
							"    $\"finishTime\",\n",
							"    $\"killed\",\n",
							"    $\"failed\",\n",
							"    $\"shuffleBytesWritten\",\n",
							"    $\"shuffleBytesRead\",\n",
							"    $\"jvmGCTime\",\n",
							"    $\"memoryBytesSpilled\",\n",
							"    $\"diskBytesSpilled\",\n",
							"    $\"taskType\",\n",
							"    $\"locality\",\n",
							"    $\"taskEndReason\",\n",
							"    expr(\"CAST( Case when fileScanTimeIndex = 0 then 0 else element_at(accumulablesValue, CAST(fileScanTimeIndex AS INTEGER)) end AS BIGINT)\").alias(\"fileScanTime\")\n",
							"    ).as(\"t\").join(query.as(\"q\"), query(\"queryId\") === taskEvents(\"queryId\") &&  query(\"startTime\") < taskEvents(\"startTime\") && query(\"endTime\") > taskEvents(\"finishTime\")).select(\"t.*\", \"q.runId\").as[Task]\n",
							"task.createOrReplaceTempView(\"task\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"with totalTasks as\n",
							"(select queryId, runId, count(1) as totalTasks from task group by 1,2 order by 1,2),\n",
							"successfulTasks as\n",
							"(select queryId, runId, count(1) as successfulTasks from task where killed = \"false\" and failed = \"false\" group by 1,2 order by 1,2)\n",
							"select tt.queryId, tt.runId, totalTasks-successfulTasks as failedTasks, successfulTasks from totalTasks tt,successfulTasks st where st.queryId = tt.queryId and st.runId = tt.runId"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/outputAnalyzer_Copy2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder 1"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## 0. Set the output location\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"//please replace the tagName in the path below for further analysis.\\nval baseOutputPath = \\\"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\\\"\\nval sparkConfPath = baseOutputPath + \\\"confJson\\\"\\nval queryMetricsPath = baseOutputPath + \\\"query-metrics/\\\"\\nval sparkEventsPath = baseOutputPath + \\\"spark-events/\\\"\\nval systemMetricsSummary = baseOutputPath + \\\"/summary/system-metrics-summary/\\\"\","
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"//please replace the tagName in the path below for further analysis.\n",
							"val baseOutputPath = \"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\"\n",
							"val sparkConfPath = baseOutputPath + \"confJson\"\n",
							"val queryMetricsPath = baseOutputPath + \"query-metrics/\"\n",
							"val sparkEventsPath = baseOutputPath + \"spark-events/\"\n",
							"val systemMetricsSummary = baseOutputPath + \"/summary/system-metrics-summary/\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 1. Spark Configurations\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkConf = spark.read.json(sparkConfPath)\\nsparkConf.select(\"spark.`spark.driver.cores`\", \"hadoop.`dfs.heartbeat.interval`\" ).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 2. queryMetrics: Get Query execution times\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Query(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    startTime: Long,\n",
							"    endTime:Long,\n",
							"    executionTimeMs: Long,\n",
							"    queryResultValidationSuccess: Boolean,\n",
							"    fileScanTimeMs: Long\n",
							"    )\n",
							"    \n",
							"    val queryMetrics = spark.read.option(\"basePath\", queryMetricsPath).json(s\"${queryMetricsPath}/*/**\")\n",
							"    val query = queryMetrics.select(\n",
							"        $\"queryId\",\n",
							"        $\"runId\",\n",
							"        $\"queryResultValidationSuccess\",\n",
							"        $\"runtimes.startTime\".alias(\"startTime\"),\n",
							"        $\"runTimes.endTime\".alias(\"endTime\"),\n",
							"        $\"runTimes.executionTimeMs\".alias(\"executionTimeMs\"),\n",
							"        $\"fileScanTimeMs\".alias(\"fileScanTimeMs\")\n",
							"        ).as[Query]\n",
							"        \n",
							"    queryMetrics.show(5)\n",
							"    query.createOrReplaceTempView(\"query\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"SELECT queryId, min(executionTimeMs), max(executionTimeMs) from query group by 1 order by 1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.0 Spark Events \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkEvents = spark.read.option(\"basePath\", sparkEventsPath).json(s\"${sparkEventsPath}/*/**\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.1 Spark Events: Task details\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Task(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    taskId: Long,\n",
							"    taskAttemptId: Long,\n",
							"    stageId: Long,\n",
							"    stageAttemptId: Long,\n",
							"    executorId: String,\n",
							"    host: String,\n",
							"    startTime: Long,\n",
							"    finishTime: Long,\n",
							"    killed: Boolean,\n",
							"    failed: Boolean,\n",
							"    shuffleBytesWritten: Long,\n",
							"    shuffleBytesRead: Long,\n",
							"    jvmGCTime: Long,\n",
							"    memoryBytesSpilled: Long,\n",
							"    diskBytesSpilled: Long,\n",
							"    taskType: String,\n",
							"    locality: String,\n",
							"    taskEndReason: String,\n",
							"    fileScanTime: Long)\n",
							"val taskEvents = sparkEvents.filter($\"Event\" === \"SparkListenerTaskEnd\").select(\n",
							"    $\"queryId\".alias(\"queryId\"),\n",
							"    $\"Task Info.Task ID\".alias(\"taskId\"),\n",
							"    $\"Task Info.Attempt\"alias(\"taskAttemptId\"),\n",
							"    $\"Stage ID\".alias(\"stageId\"),\n",
							"    $\"Stage Attempt ID\".alias(\"stageAttemptId\"),\n",
							"    $\"Task Info.Executor ID\".alias(\"executorId\"),\n",
							"    $\"Task Info.Host\".alias(\"host\"),\n",
							"    $\"Task Info.Launch Time\".alias(\"startTime\"),\n",
							"    $\"Task Info.Finish Time\".alias(\"finishTime\"),\n",
							"    $\"Task Info.Killed\".alias(\"killed\"),\n",
							"    $\"Task Info.Failed\".alias(\"failed\"),\n",
							"    $\"Task Metrics.Shuffle Write Metrics.Shuffle Bytes Written\".alias(\"shuffleBytesWritten\"),\n",
							"    ($\"Task Metrics.Shuffle Read Metrics.Remote Bytes Read\" + $\"Task Metrics.Shuffle Read Metrics.Local Bytes Read\").alias(\"shuffleBytesRead\\),\n",
							"    $\"Task Metrics.JVM GC Time\".alias(\"jvmGCTime\"),\n",
							"    $\"Task Metrics.Memory Bytes Spilled\".alias(\"memoryBytesSpilled\"),\n",
							"    $\"Task Metrics.Disk Bytes Spilled\".alias(\"diskBytesSpilled\") ,\n",
							"    $\"Task Type\".alias(\"taskType\"),\n",
							"    $\"Task Info.Locality\".alias(\"locality\"),\n",
							"    $\"Task End Reason\".alias(\"taskEndReason\"),\n",
							"    $\"Task Info.Accumulables.Value\".alias(\"accumulablesValue\"),\n",
							"    array_position($\"Task Info.Accumulables.Name\", \"scan time total (min, med, max)\").alias(\"fileScanTimeIndex\"))\n",
							"val task = taskEvents.select(\n",
							"    $\"queryId\",\n",
							"    $\"taskId\",\n",
							"    $\"taskAttemptId\",\n",
							"    $\"stageId\",\n",
							"    $\"stageAttemptId\",\n",
							"    $\"executorId\",\n",
							"    $\"host\",\n",
							"    $\"startTime\",\n",
							"    $\"finishTime\",\n",
							"    $\"killed\",\n",
							"    $\"failed\",\n",
							"    $\"shuffleBytesWritten\",\n",
							"    $\"shuffleBytesRead\",\n",
							"    $\"jvmGCTime\",\n",
							"    $\"memoryBytesSpilled\",\n",
							"    $\"diskBytesSpilled\",\n",
							"    $\"taskType\",\n",
							"    $\"locality\",\n",
							"    $\"taskEndReason\",\n",
							"    expr(\"CAST( Case when fileScanTimeIndex = 0 then 0 else element_at(accumulablesValue, CAST(fileScanTimeIndex AS INTEGER)) end AS BIGINT)\").alias(\"fileScanTime\")\n",
							"    ).as(\"t\").join(query.as(\"q\"), query(\"queryId\") === taskEvents(\"queryId\") &&  query(\"startTime\") < taskEvents(\"startTime\") && query(\"endTime\") > taskEvents(\"finishTime\")).select(\"t.*\", \"q.runId\").as[Task]\n",
							"task.createOrReplaceTempView(\"task\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"with totalTasks as\n",
							"(select queryId, runId, count(1) as totalTasks from task group by 1,2 order by 1,2),\n",
							"successfulTasks as\n",
							"(select queryId, runId, count(1) as successfulTasks from task where killed = \"false\" and failed = \"false\" group by 1,2 order by 1,2)\n",
							"select tt.queryId, tt.runId, totalTasks-successfulTasks as failedTasks, successfulTasks from totalTasks tt,successfulTasks st where st.queryId = tt.queryId and st.runId = tt.runId"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/outputAnalyzer_Copy3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder 1"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## 0. Set the output location\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"//please replace the tagName in the path below for further analysis.\\nval baseOutputPath = \\\"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\\\"\\nval sparkConfPath = baseOutputPath + \\\"confJson\\\"\\nval queryMetricsPath = baseOutputPath + \\\"query-metrics/\\\"\\nval sparkEventsPath = baseOutputPath + \\\"spark-events/\\\"\\nval systemMetricsSummary = baseOutputPath + \\\"/summary/system-metrics-summary/\\\"\","
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"//please replace the tagName in the path below for further analysis.\n",
							"val baseOutputPath = \"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\"\n",
							"val sparkConfPath = baseOutputPath + \"confJson\"\n",
							"val queryMetricsPath = baseOutputPath + \"query-metrics/\"\n",
							"val sparkEventsPath = baseOutputPath + \"spark-events/\"\n",
							"val systemMetricsSummary = baseOutputPath + \"/summary/system-metrics-summary/\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 1. Spark Configurations\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkConf = spark.read.json(sparkConfPath)\\nsparkConf.select(\"spark.`spark.driver.cores`\", \"hadoop.`dfs.heartbeat.interval`\" ).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 2. queryMetrics: Get Query execution times\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Query(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    startTime: Long,\n",
							"    endTime:Long,\n",
							"    executionTimeMs: Long,\n",
							"    queryResultValidationSuccess: Boolean,\n",
							"    fileScanTimeMs: Long\n",
							"    )\n",
							"    \n",
							"    val queryMetrics = spark.read.option(\"basePath\", queryMetricsPath).json(s\"${queryMetricsPath}/*/**\")\n",
							"    val query = queryMetrics.select(\n",
							"        $\"queryId\",\n",
							"        $\"runId\",\n",
							"        $\"queryResultValidationSuccess\",\n",
							"        $\"runtimes.startTime\".alias(\"startTime\"),\n",
							"        $\"runTimes.endTime\".alias(\"endTime\"),\n",
							"        $\"runTimes.executionTimeMs\".alias(\"executionTimeMs\"),\n",
							"        $\"fileScanTimeMs\".alias(\"fileScanTimeMs\")\n",
							"        ).as[Query]\n",
							"        \n",
							"    queryMetrics.show(5)\n",
							"    query.createOrReplaceTempView(\"query\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"SELECT queryId, min(executionTimeMs), max(executionTimeMs) from query group by 1 order by 1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.0 Spark Events \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkEvents = spark.read.option(\"basePath\", sparkEventsPath).json(s\"${sparkEventsPath}/*/**\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.1 Spark Events: Task details\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Task(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    taskId: Long,\n",
							"    taskAttemptId: Long,\n",
							"    stageId: Long,\n",
							"    stageAttemptId: Long,\n",
							"    executorId: String,\n",
							"    host: String,\n",
							"    startTime: Long,\n",
							"    finishTime: Long,\n",
							"    killed: Boolean,\n",
							"    failed: Boolean,\n",
							"    shuffleBytesWritten: Long,\n",
							"    shuffleBytesRead: Long,\n",
							"    jvmGCTime: Long,\n",
							"    memoryBytesSpilled: Long,\n",
							"    diskBytesSpilled: Long,\n",
							"    taskType: String,\n",
							"    locality: String,\n",
							"    taskEndReason: String,\n",
							"    fileScanTime: Long)\n",
							"val taskEvents = sparkEvents.filter($\"Event\" === \"SparkListenerTaskEnd\").select(\n",
							"    $\"queryId\".alias(\"queryId\"),\n",
							"    $\"Task Info.Task ID\".alias(\"taskId\"),\n",
							"    $\"Task Info.Attempt\"alias(\"taskAttemptId\"),\n",
							"    $\"Stage ID\".alias(\"stageId\"),\n",
							"    $\"Stage Attempt ID\".alias(\"stageAttemptId\"),\n",
							"    $\"Task Info.Executor ID\".alias(\"executorId\"),\n",
							"    $\"Task Info.Host\".alias(\"host\"),\n",
							"    $\"Task Info.Launch Time\".alias(\"startTime\"),\n",
							"    $\"Task Info.Finish Time\".alias(\"finishTime\"),\n",
							"    $\"Task Info.Killed\".alias(\"killed\"),\n",
							"    $\"Task Info.Failed\".alias(\"failed\"),\n",
							"    $\"Task Metrics.Shuffle Write Metrics.Shuffle Bytes Written\".alias(\"shuffleBytesWritten\"),\n",
							"    ($\"Task Metrics.Shuffle Read Metrics.Remote Bytes Read\" + $\"Task Metrics.Shuffle Read Metrics.Local Bytes Read\").alias(\"shuffleBytesRead\\),\n",
							"    $\"Task Metrics.JVM GC Time\".alias(\"jvmGCTime\"),\n",
							"    $\"Task Metrics.Memory Bytes Spilled\".alias(\"memoryBytesSpilled\"),\n",
							"    $\"Task Metrics.Disk Bytes Spilled\".alias(\"diskBytesSpilled\") ,\n",
							"    $\"Task Type\".alias(\"taskType\"),\n",
							"    $\"Task Info.Locality\".alias(\"locality\"),\n",
							"    $\"Task End Reason\".alias(\"taskEndReason\"),\n",
							"    $\"Task Info.Accumulables.Value\".alias(\"accumulablesValue\"),\n",
							"    array_position($\"Task Info.Accumulables.Name\", \"scan time total (min, med, max)\").alias(\"fileScanTimeIndex\"))\n",
							"val task = taskEvents.select(\n",
							"    $\"queryId\",\n",
							"    $\"taskId\",\n",
							"    $\"taskAttemptId\",\n",
							"    $\"stageId\",\n",
							"    $\"stageAttemptId\",\n",
							"    $\"executorId\",\n",
							"    $\"host\",\n",
							"    $\"startTime\",\n",
							"    $\"finishTime\",\n",
							"    $\"killed\",\n",
							"    $\"failed\",\n",
							"    $\"shuffleBytesWritten\",\n",
							"    $\"shuffleBytesRead\",\n",
							"    $\"jvmGCTime\",\n",
							"    $\"memoryBytesSpilled\",\n",
							"    $\"diskBytesSpilled\",\n",
							"    $\"taskType\",\n",
							"    $\"locality\",\n",
							"    $\"taskEndReason\",\n",
							"    expr(\"CAST( Case when fileScanTimeIndex = 0 then 0 else element_at(accumulablesValue, CAST(fileScanTimeIndex AS INTEGER)) end AS BIGINT)\").alias(\"fileScanTime\")\n",
							"    ).as(\"t\").join(query.as(\"q\"), query(\"queryId\") === taskEvents(\"queryId\") &&  query(\"startTime\") < taskEvents(\"startTime\") && query(\"endTime\") > taskEvents(\"finishTime\")).select(\"t.*\", \"q.runId\").as[Task]\n",
							"task.createOrReplaceTempView(\"task\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"with totalTasks as\n",
							"(select queryId, runId, count(1) as totalTasks from task group by 1,2 order by 1,2),\n",
							"successfulTasks as\n",
							"(select queryId, runId, count(1) as successfulTasks from task where killed = \"false\" and failed = \"false\" group by 1,2 order by 1,2)\n",
							"select tt.queryId, tt.runId, totalTasks-successfulTasks as failedTasks, successfulTasks from totalTasks tt,successfulTasks st where st.queryId = tt.queryId and st.runId = tt.runId"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xiaolelTest')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"println(\"hello spark print!\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"throw new Exception(\"Hey Spark Exception\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"throw new Error(\"Hey Spark Error!\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"1/0"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"System.err.println(\"Hey Spark Err Println!\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"hello, python print\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"raise Exception(\"python exception!!\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"1/0"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"import sys\n",
							"sys.stderr.write(\"python stderr write\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"source": [
							"sys.env.toList.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"spark.conf.getAll.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"import scala.collection.JavaConverters._\n",
							"import scala.collection.immutable.Seq\n",
							"spark.sparkContext.hadoopConfiguration.asScala.foreach(elem => {\n",
							"    println(\"\" + elem.getKey() + \" \\t\" + elem.getValue())\n",
							"})"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.log4j.LogManager\n",
							"val log = LogManager.getRootLogger\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"log.info(\"test log in driver\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"import com.microsoft.spark.notebook.visualization.displayHTML\n",
							"\n",
							"displayHTML(\"hello world!\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"import com.microsoft.spark.notebook.visualization.displayHTML\n",
							"\n",
							"displayHTML(\"<h1>My First Heading</h1><p>My first paragraph.</p>\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\n",
							"val deltaTablePath = s\"/delta/delta-table-$sessionId\";\n",
							"val data = spark.range(0, 5)\n",
							"data.write.format(\"delta\").save(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"val another = spark.read.load(deltaTablePath)\n",
							"another.count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"another.write.csv(\"/xiaolel/test1\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"val rdd = sc.textFile(\"abfss://mydefault@ltianscusgen2.dfs.core.windows.net/xiaolel/test1/part-00004-c914640a-2e14-4687-9f5e-457ba4ebff8f-c000.csv\")\n",
							"rdd.count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"rdd.saveAsTextFile(\"/xiaolel/rddoutputtest\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xiaolelTest1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"sys.env.toList.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.conf.getAll.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"import scala.collection.JavaConverters._\n",
							"import scala.collection.immutable.Seq\n",
							"spark.sparkContext.hadoopConfiguration.asScala.foreach(elem => {\n",
							"    println(\"\" + elem.getKey() + \" \\t\" + elem.getValue())\n",
							"})"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.log4j.LogManager\n",
							"val log = LogManager.getRootLogger\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"log.info(\"test log in driver\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\n",
							"val deltaTablePath = s\"/delta/delta-table-$sessionId\";\n",
							"val anotherpath = s\"wasbs://historyevent@pyis63h7gq2ieyy6lzovj6yk.blob.core.windows.net/test\";\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"data.write.format(\"delta\").save(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xiaolelTest1_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"sys.env.toList.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.conf.getAll.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"import scala.collection.JavaConverters._\n",
							"import scala.collection.immutable.Seq\n",
							"spark.sparkContext.hadoopConfiguration.asScala.foreach(elem => {\n",
							"    println(\"\" + elem.getKey() + \" \\t\" + elem.getValue())\n",
							"})"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.log4j.LogManager\n",
							"val log = LogManager.getRootLogger\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"log.info(\"test log in driver\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\n",
							"val deltaTablePath = s\"/delta/delta-table-$sessionId\";\n",
							"val anotherpath = s\"wasbs://historyevent@pyis63h7gq2ieyy6lzovj6yk.blob.core.windows.net/test\";\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"data.write.format(\"delta\").save(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xiaolelTest1_Copy1_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"sys.env.toList.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.conf.getAll.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"import scala.collection.JavaConverters._\n",
							"import scala.collection.immutable.Seq\n",
							"spark.sparkContext.hadoopConfiguration.asScala.foreach(elem => {\n",
							"    println(\"\" + elem.getKey() + \" \\t\" + elem.getValue())\n",
							"})"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.log4j.LogManager\n",
							"val log = LogManager.getRootLogger\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"log.info(\"test log in driver\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\n",
							"val deltaTablePath = s\"/delta/delta-table-$sessionId\";\n",
							"val anotherpath = s\"wasbs://historyevent@pyis63h7gq2ieyy6lzovj6yk.blob.core.windows.net/test\";\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"data.write.format(\"delta\").save(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/KQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "Select * from 1234",
					"metadata": {
						"language": "kql"
					},
					"currentConnection": {
						"poolName": "kustobg3pool1",
						"databaseName": ""
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/KQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "Without pool",
					"metadata": {
						"language": "kql"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		}
	]
}
