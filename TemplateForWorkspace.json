{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "dantestpara"
		},
		"AzureBlobStorage1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage1'"
		},
		"AzureDataLakeStorage1_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1'"
		},
		"AzureDataLakeStorage1116_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1116'"
		},
		"AzureDataLakeStorage1117_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1117'"
		},
		"AzureDataLakeStorage1119_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1119'"
		},
		"AzureMySql1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureMySql1'"
		},
		"AzureSqlDatabase1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlDatabase1'"
		},
		"AzureSynapseAnalytics1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSynapseAnalytics1'"
		},
		"Gen2112501_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'Gen2112501'"
		},
		"Gen21217_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'Gen21217'"
		},
		"Kusto112501_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'Kusto112501'"
		},
		"dancicdtest-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'dancicdtest-WorkspaceDefaultSqlServer'"
		},
		"dangitbugbash-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'dangitbugbash-WorkspaceDefaultSqlServer'"
		},
		"dansynapsebugbash-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'dansynapsebugbash-WorkspaceDefaultSqlServer'"
		},
		"dantestpara-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'dantestpara-WorkspaceDefaultSqlServer'"
		},
		"gen21122_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'gen21122'"
		},
		"kusto1122_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'kusto1122'"
		},
		"kusto1123_servicePrincipalKey": {
			"type": "secureString",
			"metadata": "Secure string for 'servicePrincipalKey' of 'kusto1123'"
		},
		"ltianscusworkspace-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ltianscusworkspace-WorkspaceDefaultSqlServer'"
		},
		"testpewsdeploy-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'testpewsdeploy-WorkspaceDefaultSqlServer'"
		},
		"workspacedeploy-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'workspacedeploy-WorkspaceDefaultSqlServer'"
		},
		"AzureBlobStorage1_properties_typeProperties_connectionString": {
			"type": "string"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://hozhaotestgen2.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1_properties_typeProperties_accountKey": {
			"type": "string"
		},
		"AzureDataLakeStorage1116_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://storagegen2qingtest.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1116_properties_typeProperties_accountKey": {
			"type": "string"
		},
		"AzureDataLakeStorage1117_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://storagegen2qingtest.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1117_properties_typeProperties_accountKey": {
			"type": "string"
		},
		"AzureDataLakeStorage1118_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://dansynapsestorage.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1118_properties_typeProperties_accountKey": {
			"type": "object",
			"defaultValue": {
				"type": "AzureKeyVaultSecret",
				"store": {
					"referenceName": "AzureKeyVault1",
					"type": "LinkedServiceReference"
				},
				"secretName": "name1"
			}
		},
		"AzureDataLakeStorage1119_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://accessibilitytest.dfs.core.windows.net"
		},
		"AzureDataLakeStorage1119_properties_typeProperties_accountKey": {
			"type": "string"
		},
		"AzureKeyVault1_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://a365-e2e.vault.azure.net/"
		},
		"AzureMySql1_properties_typeProperties_connectionString": {
			"type": "string"
		},
		"AzureSqlDatabase1_properties_typeProperties_connectionString": {
			"type": "string"
		},
		"AzureSynapseAnalytics1_properties_typeProperties_connectionString": {
			"type": "string"
		},
		"Gen1112501_properties_typeProperties_dataLakeStoreUri": {
			"type": "string",
			"defaultValue": "https://storagegen1qingtest.azuredatalakestore.net/webhdfs/v1"
		},
		"Gen1112501_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"Gen1112501_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3"
		},
		"Gen1112501_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "bigdataqa"
		},
		"Gen2112501_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://storagegen2qingtest.dfs.core.windows.net"
		},
		"Gen2112501_properties_typeProperties_accountKey": {
			"type": "string"
		},
		"Gen21217_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://storagegen2qingtest.dfs.core.windows.net"
		},
		"Gen21217_properties_typeProperties_accountKey": {
			"type": "string"
		},
		"Kusto112501_properties_typeProperties_endpoint": {
			"type": "string",
			"defaultValue": "https://kustodemo.westus2.kusto.windows.net"
		},
		"Kusto112501_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"Kusto112501_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "ac3a0e33-1db1-48ba-9b19-24ce0330ff5c"
		},
		"Kusto112501_properties_typeProperties_servicePrincipalKey": {
			"type": "string"
		},
		"Kusto112501_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "cdndemo"
		},
		"PowerBIWorkspace1_properties_typeProperties_workspaceID": {
			"type": "string",
			"defaultValue": "28a0e6b5-f83b-47c9-aa9f-eaab6b8a2bf9"
		},
		"PowerBIWorkspace1_properties_typeProperties_tenantID": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"dancicdtest-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString": {
			"type": "string"
		},
		"dancicdtest-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://accessibilitytest.dfs.core.windows.net"
		},
		"dangitbugbash-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString": {
			"type": "string"
		},
		"dangitbugbash-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://dansynapsestroage.dfs.core.windows.net"
		},
		"dansynapsebugbash-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString": {
			"type": "string"
		},
		"dansynapsebugbash-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://dansynapsestroage.dfs.core.windows.net"
		},
		"dantestpara-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString": {
			"type": "string"
		},
		"dantestpara-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://dansynapsestoragev2.dfs.core.windows.net"
		},
		"gen11123_properties_typeProperties_dataLakeStoreUri": {
			"type": "string",
			"defaultValue": "https://ruxuadlsgen1.azuredatalakestore.net/webhdfs/v1"
		},
		"gen11123_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"gen11123_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3"
		},
		"gen11123_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "ruxurg"
		},
		"gen21122_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://storagegen2qingtest.dfs.core.windows.net"
		},
		"gen21122_properties_typeProperties_accountKey": {
			"type": "string"
		},
		"kusto1122_properties_typeProperties_endpoint": {
			"type": "string",
			"defaultValue": "https://lirsuntest.southeastasia.kusto.windows.net"
		},
		"kusto1122_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"kusto1122_properties_typeProperties_servicePrincipalId": {
			"type": "int",
			"defaultValue": 123
		},
		"kusto1122_properties_typeProperties_servicePrincipalKey": {
			"type": "string"
		},
		"kusto1122_properties_typeProperties_database": {
			"type": "int",
			"defaultValue": 789
		},
		"kusto1123_properties_typeProperties_endpoint": {
			"type": "string",
			"defaultValue": "https://lirsuntest.southeastasia.kusto.windows.net"
		},
		"kusto1123_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"kusto1123_properties_typeProperties_servicePrincipalId": {
			"type": "int",
			"defaultValue": 34
		},
		"kusto1123_properties_typeProperties_servicePrincipalKey": {
			"type": "string"
		},
		"kusto1123_properties_typeProperties_database": {
			"type": "int",
			"defaultValue": 87654
		},
		"ltianscusworkspace-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString": {
			"type": "string"
		},
		"ltianscusworkspace-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://ltianscusgen2.dfs.core.windows.net"
		},
		"testpewsdeploy-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString": {
			"type": "string"
		},
		"testpewsdeploy-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://dansynapsestoragev2.dfs.core.windows.net"
		},
		"workspacedeploy-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString": {
			"type": "string"
		},
		"workspacedeploy-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://dansynapsestoragev2.dfs.core.windows.net"
		},
		"Credential1_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"Credential1_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "ij"
		},
		"Credential1_properties_typeProperties_servicePrincipalKey": {
			"type": "object",
			"defaultValue": {
				"type": "AzureKeyVaultSecret",
				"store": {
					"referenceName": "AzureKeyVault1",
					"type": "LinkedServiceReference"
				},
				"secretName": "123"
			}
		},
		"Credential2_properties_typeProperties_tenant": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"Credential2_properties_typeProperties_servicePrincipalId": {
			"type": "string",
			"defaultValue": "72f988bf-86f1-41af-91ab-2d7cd011db47"
		},
		"Credential2_properties_typeProperties_servicePrincipalKey": {
			"type": "object",
			"defaultValue": {
				"type": "AzureKeyVaultSecret",
				"store": {
					"referenceName": "AzureKeyVault1",
					"type": "LinkedServiceReference"
				},
				"secretName": "name"
			}
		},
		"sql script 1_properties_content_currentConnection_databaseName": {
			"type": "string"
		},
		"sql script 1_properties_content_currentConnection_poolName": {
			"type": "string"
		},
		"sql script 1_Copy1_properties_content_currentConnection_databaseName": {
			"type": "string"
		},
		"sql script 1_Copy1_properties_content_currentConnection_poolName": {
			"type": "string"
		},
		"SQL script 2_properties_content_currentConnection_databaseName": {
			"type": "string"
		},
		"SQL script 2_properties_content_currentConnection_poolName": {
			"type": "string"
		},
		"SQL script 3_properties_content_currentConnection_databaseName": {
			"type": "string"
		},
		"SQL script 3_properties_content_currentConnection_poolName": {
			"type": "string"
		},
		"Sql script1_properties_content_currentConnection_databaseName": {
			"type": "string"
		},
		"Sql script1_properties_content_currentConnection_poolName": {
			"type": "string"
		},
		"sql script 1_Copy1_Copy1_properties_content_currentConnection_databaseName": {
			"type": "string"
		},
		"sql script 1_Copy1_Copy1_properties_content_currentConnection_poolName": {
			"type": "string"
		},
		"Synapse Notebook1_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"Synapse Notebook_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"DebugFailCase3_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "catest"
		},
		"DebugFailCase3_Copy1_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "catest"
		},
		"DebugFailCase5_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "catest"
		},
		"DebugFailCase5_Copy2_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "catest"
		},
		"DebugFailCase5_Copy3_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "catest"
		},
		"DebugFailCase5_Copy4_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "catest"
		},
		"DebugFailCase6_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "catest"
		},
		"DebugFailCase7_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"DebugFailCase8_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "catest"
		},
		"DebugFailCase9_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "catest"
		},
		"DebugFailCases4_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"Getting Started with Delta Lake_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"InputOutputTest_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"InputOutputTest_Copy1_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"Synapse Notebook2_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"displayhtmlnotebook_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"displayhtmlnotebook_Copy1_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"displaynotebook_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"matplotlibnotebook_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"matplotlibnotebook_Copy1_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"mssparkutilsnotebook_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "smallpool"
		},
		"outputAnalyzer_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"outputAnalyzer_Copy1_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"outputAnalyzer_Copy2_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"outputAnalyzer_Copy3_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"xiaolelTest_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"xiaolelTest1_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"xiaolelTest1_Copy1_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		},
		"xiaolelTest1_Copy1_Copy1_properties_bigDataPool_referenceName": {
			"type": "string",
			"defaultValue": "origin"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/CopyPipeline_7e9')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy_7e9 ",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Source",
								"value": "tempdata/parquet/parquet-table-749572/"
							},
							{
								"name": "Destination",
								"value": "workspace//"
							}
						],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFileName": "*",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings",
									"skipLineCount": 0
								}
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings",
									"filePattern": "arrayOfObjects"
								}
							},
							"enableStaging": false,
							"validateDataConsistency": false,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"type": "String",
											"ordinal": 1
										},
										"sink": {
											"path": "$['Column1']"
										}
									},
									{
										"source": {
											"type": "String",
											"ordinal": 2
										},
										"sink": {
											"path": "$['Column2']"
										}
									},
									{
										"source": {
											"type": "String",
											"ordinal": 3
										},
										"sink": {
											"path": "$['Column3']"
										}
									},
									{
										"source": {
											"type": "String",
											"ordinal": 4
										},
										"sink": {
											"path": "$['Column4']"
										}
									},
									{
										"source": {
											"type": "String",
											"ordinal": 5
										},
										"sink": {
											"path": "$['Column5']"
										}
									},
									{
										"source": {
											"type": "String",
											"ordinal": 6
										},
										"sink": {
											"path": "$['Column6']"
										}
									}
								]
							}
						},
						"inputs": [
							{
								"referenceName": "SourceDataset_7e9",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDataset_7e9",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2021-08-05T05:57:50Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/SourceDataset_7e9')]",
				"[concat(variables('workspaceId'), '/datasets/DestinationDataset_7e9')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook1",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "DebugFailCase7",
								"type": "NotebookReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2021-07-26T14:27:10Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/DebugFailCase7')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Spark job definition1",
						"type": "SparkJob",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"sparkJob": {
								"referenceName": "DebugCase4",
								"type": "SparkJobDefinitionReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2021-07-23T08:21:19Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkJobDefinitions/DebugCase4')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 3')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Execute Pipeline1",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Pipeline 1",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2021-07-23T08:21:15Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Pipeline 1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 4')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "SQL pool stored procedure1",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "pool1",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[Procedure]"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2021-08-10T08:56:29Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/pool1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 4_copy1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "SQL pool stored procedure1",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "pool1",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[Procedure]"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2021-08-10T08:56:33Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/pool1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 4_copy2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "SQL pool stored procedure1",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "pool1",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[Procedure1]"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2021-08-10T08:56:31Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/pool1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 5')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "SQL pool stored procedure1",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "pool1",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[Procedure]"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2021-08-10T08:56:33Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/pool1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/web')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Web1",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://code.visualstudio.com/docs/editor/workspaces",
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {
								"hearder": "1"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "New folder"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-23T08:21:18Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_7e9')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "dantestpara-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "workspace"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/dantestpara-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_7e9')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "dantestpara-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "parquet/parquet-table-749572",
						"fileSystem": "tempdata"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": false,
					"quoteChar": "\""
				},
				"schema": [
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/dantestpara-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": {
						"type": "SecureString",
						"value": "[parameters('AzureBlobStorage1_properties_typeProperties_connectionString')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1_properties_typeProperties_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1116')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1116_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1116_properties_typeProperties_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1117')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1117_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1117_properties_typeProperties_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1118')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1118_properties_typeProperties_url')]",
					"accountKey": "[parameters('AzureDataLakeStorage1118_properties_typeProperties_accountKey')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1119')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1119_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1119_properties_typeProperties_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureKeyVault1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('AzureKeyVault1_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureMySql1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureMySql",
				"typeProperties": {
					"connectionString": {
						"type": "SecureString",
						"value": "[parameters('AzureMySql1_properties_typeProperties_connectionString')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDatabase1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": {
						"type": "SecureString",
						"value": "[parameters('AzureSqlDatabase1_properties_typeProperties_connectionString')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalytics1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": {
						"type": "SecureString",
						"value": "[parameters('AzureSynapseAnalytics1_properties_typeProperties_connectionString')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Gen1112501')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataLakeStore",
				"typeProperties": {
					"dataLakeStoreUri": "[parameters('Gen1112501_properties_typeProperties_dataLakeStoreUri')]",
					"tenant": "[parameters('Gen1112501_properties_typeProperties_tenant')]",
					"subscriptionId": "[parameters('Gen1112501_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('Gen1112501_properties_typeProperties_resourceGroupName')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Gen2112501')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "1204",
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('Gen2112501_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('Gen2112501_properties_typeProperties_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Gen21217')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('Gen21217_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('Gen21217_properties_typeProperties_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Kusto112501')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "1204",
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "[parameters('Kusto112501_properties_typeProperties_endpoint')]",
					"tenant": "[parameters('Kusto112501_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('Kusto112501_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('Kusto112501_properties_typeProperties_servicePrincipalKey')]"
					},
					"database": "[parameters('Kusto112501_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PowerBIWorkspace1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "PowerBIWorkspace",
				"typeProperties": {
					"workspaceID": "[parameters('PowerBIWorkspace1_properties_typeProperties_workspaceID')]",
					"tenantID": "[parameters('PowerBIWorkspace1_properties_typeProperties_tenantID')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dancicdtest-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": {
						"type": "SecureString",
						"value": "[parameters('dancicdtest-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dancicdtest-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('dancicdtest-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dangitbugbash-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": {
						"type": "SecureString",
						"value": "[parameters('dangitbugbash-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dangitbugbash-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('dangitbugbash-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dansynapsebugbash-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": {
						"type": "SecureString",
						"value": "[parameters('dansynapsebugbash-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dansynapsebugbash-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('dansynapsebugbash-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dantestpara-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": {
						"type": "SecureString",
						"value": "[parameters('dantestpara-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dantestpara-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('dantestpara-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/gen11123')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataLakeStore",
				"typeProperties": {
					"dataLakeStoreUri": "[parameters('gen11123_properties_typeProperties_dataLakeStoreUri')]",
					"tenant": "[parameters('gen11123_properties_typeProperties_tenant')]",
					"subscriptionId": "[parameters('gen11123_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('gen11123_properties_typeProperties_resourceGroupName')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/gen21122')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "test",
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('gen21122_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('gen21122_properties_typeProperties_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kusto1122')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "test1",
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "[parameters('kusto1122_properties_typeProperties_endpoint')]",
					"tenant": "[parameters('kusto1122_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('kusto1122_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('kusto1122_properties_typeProperties_servicePrincipalKey')]"
					},
					"database": "[parameters('kusto1122_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kusto1123')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "[parameters('kusto1123_properties_typeProperties_endpoint')]",
					"tenant": "[parameters('kusto1123_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('kusto1123_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": {
						"type": "SecureString",
						"value": "[parameters('kusto1123_properties_typeProperties_servicePrincipalKey')]"
					},
					"database": "[parameters('kusto1123_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ltianscusworkspace-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": {
						"type": "SecureString",
						"value": "[parameters('ltianscusworkspace-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ltianscusworkspace-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ltianscusworkspace-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testpewsdeploy-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": {
						"type": "SecureString",
						"value": "[parameters('testpewsdeploy-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testpewsdeploy-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('testpewsdeploy-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/workspacedeploy-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": {
						"type": "SecureString",
						"value": "[parameters('workspacedeploy-WorkspaceDefaultSqlServer_properties_typeProperties_connectionString')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/workspacedeploy-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('workspacedeploy-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 1')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Pipeline 1",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 15,
						"startTime": "2021-02-10T10:18:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Pipeline 1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 2')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 15,
						"startTime": "2021-02-24T06:33:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 3')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 15,
						"startTime": "2021-07-13T15:06:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime1')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0,
							"cleanup": true
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime1125')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "South Central US",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Credential1')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ServicePrincipal",
				"typeProperties": {
					"tenant": "[parameters('Credential1_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('Credential1_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": "[parameters('Credential1_properties_typeProperties_servicePrincipalKey')]"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Credential2')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ServicePrincipal",
				"typeProperties": {
					"tenant": "[parameters('Credential2_properties_typeProperties_tenant')]",
					"servicePrincipalId": "[parameters('Credential2_properties_typeProperties_servicePrincipalId')]",
					"servicePrincipalKey": "[parameters('Credential2_properties_typeProperties_servicePrincipalKey')]"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sql script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "Begin\nwhile 1=1\nselect '123 -- testtesttesttesttesttesttest' \nend ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "[parameters('sql script 1_properties_content_currentConnection_databaseName')]",
						"poolName": "[parameters('sql script 1_properties_content_currentConnection_poolName')]"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sql script 1_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "Begin\nwhile 1=1\nselect '123 -- testtesttesttesttesttesttest'\nend   ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "[parameters('sql script 1_Copy1_properties_content_currentConnection_databaseName')]",
						"poolName": "[parameters('sql script 1_Copy1_properties_content_currentConnection_poolName')]"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "fghj",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "[parameters('SQL script 2_properties_content_currentConnection_databaseName')]",
						"poolName": "[parameters('SQL script 2_properties_content_currentConnection_poolName')]"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "USE AdventureWorks2012;\nGO\nSELECT *\nFROM Production.Product\nORDER BY Name ASC;\n-- Alternate way.\nUSE AdventureWorks2012;\nGO\nSELECT p.*\nFROM Production.Product AS p\nORDER BY Name ASC;\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "[parameters('SQL script 3_properties_content_currentConnection_databaseName')]",
						"poolName": "[parameters('SQL script 3_properties_content_currentConnection_poolName')]"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Sql script1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SQL *\nFROM spt_fallback_db  ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "[parameters('Sql script1_properties_content_currentConnection_databaseName')]",
						"poolName": "[parameters('Sql script1_properties_content_currentConnection_poolName')]"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sql script 1_Copy1_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "Begin\nwhile 1=1\nselect '123 -- testtesttesttesttesttesttest'\nend   ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "[parameters('sql script 1_Copy1_Copy1_properties_content_currentConnection_databaseName')]",
						"poolName": "[parameters('sql script 1_Copy1_Copy1_properties_content_currentConnection_poolName')]"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Synapse Notebook1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('Synapse Notebook1_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Charting in Synapse Notebook\n",
							"\n",
							"Synapse has common used data visualization packages pre installed, such as **matplotlib**, **bokeh**, **seaborn**, **altair**, **plotly**. This notebook provides examples to do data visualization using charts in Synapse notebook. \n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Matplotlib\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Line charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							" \n",
							"x  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
							"y1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\n",
							"y2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\n",
							"plt.plot(x, y1, label=\"line L\")\n",
							"plt.plot(x, y2, label=\"line H\")\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"x axis\")\n",
							"plt.ylabel(\"y axis\")\n",
							"plt.title(\"Line Graph Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Bar chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"# Look at index 4 and 6, which demonstrate overlapping cases.\n",
							"x1 = [1, 3, 4, 5, 6, 7, 9]\n",
							"y1 = [4, 7, 2, 4, 7, 8, 3]\n",
							"\n",
							"x2 = [2, 4, 6, 8, 10]\n",
							"y2 = [5, 6, 2, 6, 2]\n",
							"\n",
							"# Colors: https://matplotlib.org/api/colors_api.html\n",
							"\n",
							"plt.bar(x1, y1, label=\"Blue Bar\", color='b')\n",
							"plt.bar(x2, y2, label=\"Green Bar\", color='g')\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"bar number\")\n",
							"plt.ylabel(\"bar height\")\n",
							"plt.title(\"Bar Chart Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Histogram\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Use numpy to generate a bunch of random data in a bell curve around 5.\n",
							"n = 5 + np.random.randn(1000)\n",
							"\n",
							"m = [m for m in range(len(n))]\n",
							"plt.bar(m, n)\n",
							"plt.title(\"Raw Data\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, bins=20)\n",
							"plt.title(\"Histogram\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, cumulative=True, bins=20)\n",
							"plt.title(\"Cumulative Histogram\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Scatter chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"x1 = [2, 3, 4]\n",
							"y1 = [5, 5, 5]\n",
							"\n",
							"x2 = [1, 2, 3, 4, 5]\n",
							"y2 = [2, 3, 2, 3, 4]\n",
							"y3 = [6, 8, 7, 8, 7]\n",
							"\n",
							"# Markers: https://matplotlib.org/api/markers_api.html\n",
							"\n",
							"plt.scatter(x1, y1)\n",
							"plt.scatter(x2, y2, marker='v', color='r')\n",
							"plt.scatter(x2, y3, marker='^', color='m')\n",
							"plt.title('Scatter Plot Example')\n",
							"plt.show()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Stack plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"idxes = [ 1,  2,  3,  4,  5,  6,  7,  8,  9]\n",
							"arr1  = [23, 40, 28, 43,  8, 44, 43, 18, 17]\n",
							"arr2  = [17, 30, 22, 14, 17, 17, 29, 22, 30]\n",
							"arr3  = [15, 31, 18, 22, 18, 19, 13, 32, 39]\n",
							"\n",
							"# Adding legend for stack plots is tricky.\n",
							"plt.plot([], [], color='r', label = 'D 1')\n",
							"plt.plot([], [], color='g', label = 'D 2')\n",
							"plt.plot([], [], color='b', label = 'D 3')\n",
							"\n",
							"plt.stackplot(idxes, arr1, arr2, arr3, colors= ['r', 'g', 'b'])\n",
							"plt.title('Stack Plot Example')\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Pie charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"labels = 'S1', 'S2', 'S3'\n",
							"sections = [56, 66, 24]\n",
							"colors = ['c', 'g', 'y']\n",
							"\n",
							"plt.pie(sections, labels=labels, colors=colors,\n",
							"        startangle=90,\n",
							"        explode = (0, 0.1, 0),\n",
							"        autopct = '%1.2f%%')\n",
							"\n",
							"plt.axis('equal') # Try commenting this out.\n",
							"plt.title('Pie Chart Example')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# fill_between and alpha\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"ys = 200 + np.random.randn(100)\n",
							"x = [x for x in range(len(ys))]\n",
							"\n",
							"plt.plot(x, ys, '-')\n",
							"plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)\n",
							"\n",
							"plt.title(\"Fills and Alpha Example\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Subplotting using Subplot2grid\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"def random_plots():\n",
							"  xs = []\n",
							"  ys = []\n",
							"  \n",
							"  for i in range(20):\n",
							"    x = i\n",
							"    y = np.random.randint(10)\n",
							"    \n",
							"    xs.append(x)\n",
							"    ys.append(y)\n",
							"  \n",
							"  return xs, ys\n",
							"\n",
							"fig = plt.figure()\n",
							"ax1 = plt.subplot2grid((5, 2), (0, 0), rowspan=1, colspan=2)\n",
							"ax2 = plt.subplot2grid((5, 2), (1, 0), rowspan=3, colspan=2)\n",
							"ax3 = plt.subplot2grid((5, 2), (4, 0), rowspan=1, colspan=1)\n",
							"ax4 = plt.subplot2grid((5, 2), (4, 1), rowspan=1, colspan=1)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax1.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax2.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax3.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax4.plot(x, y)\n",
							"\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# 3D Scatter Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"from mpl_toolkits.mplot3d import axes3d\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y1 = np.random.randint(10, size=10)\n",
							"z1 = np.random.randint(10, size=10)\n",
							"\n",
							"x2 = [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\n",
							"y2 = np.random.randint(-10, 0, size=10)\n",
							"z2 = np.random.randint(10, size=10)\n",
							"\n",
							"ax.scatter(x1, y1, z1, c='b', marker='o', label='blue')\n",
							"ax.scatter(x2, y2, z2, c='g', marker='D', label='green')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Scatter Plot Example\")\n",
							"plt.legend()\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# 3D Bar Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y = np.random.randint(10, size=10)\n",
							"z = np.zeros(10)\n",
							"\n",
							"dx = np.ones(10)\n",
							"dy = np.ones(10)\n",
							"dz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"\n",
							"ax.bar3d(x, y, z, dx, dy, dz, color='g')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Bar Chart Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Wireframe Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x, y, z = axes3d.get_test_data()\n",
							"\n",
							"ax.plot_wireframe(x, y, z, rstride = 2, cstride = 2)\n",
							"\n",
							"plt.title(\"Wireframe Plot Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Seaborn\n",
							"Seaborn is a library layered on top of Matplotlib that you can use."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Scatterplot with a nice regression line fit to it, all with just one call to Seaborn's regplot.\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"import seaborn as sns\n",
							"\n",
							"# Generate some random data\n",
							"num_points = 20\n",
							"# x will be 5, 6, 7... but also twiddled randomly\n",
							"x = 5 + np.arange(num_points) + np.random.randn(num_points)\n",
							"# y will be 10, 11, 12... but twiddled even more randomly\n",
							"y = 10 + np.arange(num_points) + 5 * np.random.randn(num_points)\n",
							"sns.regplot(x, y)\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Seanborn heatmap\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Make a 10 x 10 heatmap of some random data\n",
							"side_length = 10\n",
							"# Start with a 10 x 10 matrix with values randomized around 5\n",
							"data = 5 + np.random.randn(side_length, side_length)\n",
							"# The next two lines make the values larger as we get closer to (9, 9)\n",
							"data += np.arange(side_length)\n",
							"data += np.reshape(np.arange(side_length), (side_length, 1))\n",
							"# Generate the heatmap\n",
							"sns.heatmap(data)\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Bokeh\n",
							"You can render HTML or interactive libraries, like **bokeh**, using the **displayHTML()**.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import numpy as np\n",
							"from bokeh.plotting import figure, show\n",
							"from bokeh.io import output_notebook\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"\n",
							"N = 4000\n",
							"x = np.random.random(size=N) * 100\n",
							"y = np.random.random(size=N) * 100\n",
							"radii = np.random.random(size=N) * 1.5\n",
							"colors = [\"#%02x%02x%02x\" % (r, g, 150) for r, g in zip(np.floor(50+2*x).astype(int), np.floor(30+2*y).astype(int))]\n",
							"\n",
							"p = figure()\n",
							"p.circle(x, y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None)\n",
							"show(p)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Plotting glyphs over a map using bokeh.\n",
							"\n",
							"from bokeh.plotting import figure, output_file\n",
							"from bokeh.tile_providers import get_provider, Vendors\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"from bokeh.models import ColumnDataSource\n",
							"\n",
							"tile_provider = get_provider(Vendors.CARTODBPOSITRON)\n",
							"\n",
							"# range bounds supplied in web mercator coordinates\n",
							"p = figure(x_range=(-9000000,-8000000), y_range=(4000000,5000000),\n",
							"           x_axis_type=\"mercator\", y_axis_type=\"mercator\")\n",
							"p.add_tile(tile_provider)\n",
							"\n",
							"# plot datapoints on the map\n",
							"source = ColumnDataSource(\n",
							"    data=dict(x=[ -8800000, -8500000 , -8800000],\n",
							"              y=[4200000, 4500000, 4900000])\n",
							")\n",
							"\n",
							"p.circle(x=\"x\", y=\"y\", size=15, fill_color=\"blue\", fill_alpha=0.8, source=source)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Synapse Notebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('Synapse Notebook_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Charting in Synapse Notebook\n",
							"\n",
							"Synapse has common used data visualization packages pre installed, such as **matplotlib**, **bokeh**, **seaborn**, **altair**, **plotly**. This notebook provides examples to do data visualization using charts in Synapse notebook. \n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Matplotlib\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Line charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							" \n",
							"x  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
							"y1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\n",
							"y2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\n",
							"plt.plot(x, y1, label=\"line L\")\n",
							"plt.plot(x, y2, label=\"line H\")\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"x axis\")\n",
							"plt.ylabel(\"y axis\")\n",
							"plt.title(\"Line Graph Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Bar chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"# Look at index 4 and 6, which demonstrate overlapping cases.\n",
							"x1 = [1, 3, 4, 5, 6, 7, 9]\n",
							"y1 = [4, 7, 2, 4, 7, 8, 3]\n",
							"\n",
							"x2 = [2, 4, 6, 8, 10]\n",
							"y2 = [5, 6, 2, 6, 2]\n",
							"\n",
							"# Colors: https://matplotlib.org/api/colors_api.html\n",
							"\n",
							"plt.bar(x1, y1, label=\"Blue Bar\", color='b')\n",
							"plt.bar(x2, y2, label=\"Green Bar\", color='g')\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"bar number\")\n",
							"plt.ylabel(\"bar height\")\n",
							"plt.title(\"Bar Chart Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Histogram\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Use numpy to generate a bunch of random data in a bell curve around 5.\n",
							"n = 5 + np.random.randn(1000)\n",
							"\n",
							"m = [m for m in range(len(n))]\n",
							"plt.bar(m, n)\n",
							"plt.title(\"Raw Data\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, bins=20)\n",
							"plt.title(\"Histogram\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, cumulative=True, bins=20)\n",
							"plt.title(\"Cumulative Histogram\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Scatter chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"x1 = [2, 3, 4]\n",
							"y1 = [5, 5, 5]\n",
							"\n",
							"x2 = [1, 2, 3, 4, 5]\n",
							"y2 = [2, 3, 2, 3, 4]\n",
							"y3 = [6, 8, 7, 8, 7]\n",
							"\n",
							"# Markers: https://matplotlib.org/api/markers_api.html\n",
							"\n",
							"plt.scatter(x1, y1)\n",
							"plt.scatter(x2, y2, marker='v', color='r')\n",
							"plt.scatter(x2, y3, marker='^', color='m')\n",
							"plt.title('Scatter Plot Example')\n",
							"plt.show()\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Stack plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"idxes = [ 1,  2,  3,  4,  5,  6,  7,  8,  9]\n",
							"arr1  = [23, 40, 28, 43,  8, 44, 43, 18, 17]\n",
							"arr2  = [17, 30, 22, 14, 17, 17, 29, 22, 30]\n",
							"arr3  = [15, 31, 18, 22, 18, 19, 13, 32, 39]\n",
							"\n",
							"# Adding legend for stack plots is tricky.\n",
							"plt.plot([], [], color='r', label = 'D 1')\n",
							"plt.plot([], [], color='g', label = 'D 2')\n",
							"plt.plot([], [], color='b', label = 'D 3')\n",
							"\n",
							"plt.stackplot(idxes, arr1, arr2, arr3, colors= ['r', 'g', 'b'])\n",
							"plt.title('Stack Plot Example')\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Pie charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"labels = 'S1', 'S2', 'S3'\n",
							"sections = [56, 66, 24]\n",
							"colors = ['c', 'g', 'y']\n",
							"\n",
							"plt.pie(sections, labels=labels, colors=colors,\n",
							"        startangle=90,\n",
							"        explode = (0, 0.1, 0),\n",
							"        autopct = '%1.2f%%')\n",
							"\n",
							"plt.axis('equal') # Try commenting this out.\n",
							"plt.title('Pie Chart Example')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# fill_between and alpha\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"ys = 200 + np.random.randn(100)\n",
							"x = [x for x in range(len(ys))]\n",
							"\n",
							"plt.plot(x, ys, '-')\n",
							"plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)\n",
							"\n",
							"plt.title(\"Fills and Alpha Example\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Subplotting using Subplot2grid\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"def random_plots():\n",
							"  xs = []\n",
							"  ys = []\n",
							"  \n",
							"  for i in range(20):\n",
							"    x = i\n",
							"    y = np.random.randint(10)\n",
							"    \n",
							"    xs.append(x)\n",
							"    ys.append(y)\n",
							"  \n",
							"  return xs, ys\n",
							"\n",
							"fig = plt.figure()\n",
							"ax1 = plt.subplot2grid((5, 2), (0, 0), rowspan=1, colspan=2)\n",
							"ax2 = plt.subplot2grid((5, 2), (1, 0), rowspan=3, colspan=2)\n",
							"ax3 = plt.subplot2grid((5, 2), (4, 0), rowspan=1, colspan=1)\n",
							"ax4 = plt.subplot2grid((5, 2), (4, 1), rowspan=1, colspan=1)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax1.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax2.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax3.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax4.plot(x, y)\n",
							"\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# 3D Scatter Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"from mpl_toolkits.mplot3d import axes3d\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y1 = np.random.randint(10, size=10)\n",
							"z1 = np.random.randint(10, size=10)\n",
							"\n",
							"x2 = [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\n",
							"y2 = np.random.randint(-10, 0, size=10)\n",
							"z2 = np.random.randint(10, size=10)\n",
							"\n",
							"ax.scatter(x1, y1, z1, c='b', marker='o', label='blue')\n",
							"ax.scatter(x2, y2, z2, c='g', marker='D', label='green')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Scatter Plot Example\")\n",
							"plt.legend()\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# 3D Bar Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y = np.random.randint(10, size=10)\n",
							"z = np.zeros(10)\n",
							"\n",
							"dx = np.ones(10)\n",
							"dy = np.ones(10)\n",
							"dz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"\n",
							"ax.bar3d(x, y, z, dx, dy, dz, color='g')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Bar Chart Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Wireframe Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x, y, z = axes3d.get_test_data()\n",
							"\n",
							"ax.plot_wireframe(x, y, z, rstride = 2, cstride = 2)\n",
							"\n",
							"plt.title(\"Wireframe Plot Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Seaborn\n",
							"Seaborn is a library layered on top of Matplotlib that you can use."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Scatterplot with a nice regression line fit to it, all with just one call to Seaborn's regplot.\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"import seaborn as sns\n",
							"\n",
							"# Generate some random data\n",
							"num_points = 20\n",
							"# x will be 5, 6, 7... but also twiddled randomly\n",
							"x = 5 + np.arange(num_points) + np.random.randn(num_points)\n",
							"# y will be 10, 11, 12... but twiddled even more randomly\n",
							"y = 10 + np.arange(num_points) + 5 * np.random.randn(num_points)\n",
							"sns.regplot(x, y)\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Seanborn heatmap\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Make a 10 x 10 heatmap of some random data\n",
							"side_length = 10\n",
							"# Start with a 10 x 10 matrix with values randomized around 5\n",
							"data = 5 + np.random.randn(side_length, side_length)\n",
							"# The next two lines make the values larger as we get closer to (9, 9)\n",
							"data += np.arange(side_length)\n",
							"data += np.reshape(np.arange(side_length), (side_length, 1))\n",
							"# Generate the heatmap\n",
							"sns.heatmap(data)\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Bokeh\n",
							"You can render HTML or interactive libraries, like **bokeh**, using the **displayHTML()**.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import numpy as np\n",
							"from bokeh.plotting import figure, show\n",
							"from bokeh.io import output_notebook\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"\n",
							"N = 4000\n",
							"x = np.random.random(size=N) * 100\n",
							"y = np.random.random(size=N) * 100\n",
							"radii = np.random.random(size=N) * 1.5\n",
							"colors = [\"#%02x%02x%02x\" % (r, g, 150) for r, g in zip(np.floor(50+2*x).astype(int), np.floor(30+2*y).astype(int))]\n",
							"\n",
							"p = figure()\n",
							"p.circle(x, y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None)\n",
							"show(p)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Plotting glyphs over a map using bokeh.\n",
							"\n",
							"from bokeh.plotting import figure, output_file\n",
							"from bokeh.tile_providers import get_provider, Vendors\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"from bokeh.models import ColumnDataSource\n",
							"\n",
							"tile_provider = get_provider(Vendors.CARTODBPOSITRON)\n",
							"\n",
							"# range bounds supplied in web mercator coordinates\n",
							"p = figure(x_range=(-9000000,-8000000), y_range=(4000000,5000000),\n",
							"           x_axis_type=\"mercator\", y_axis_type=\"mercator\")\n",
							"p.add_tile(tile_provider)\n",
							"\n",
							"# plot datapoints on the map\n",
							"source = ColumnDataSource(\n",
							"    data=dict(x=[ -8800000, -8500000 , -8800000],\n",
							"              y=[4200000, 4500000, 4900000])\n",
							")\n",
							"\n",
							"p.circle(x=\"x\", y=\"y\", size=15, fill_color=\"blue\", fill_alpha=0.8, source=source)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugFailCase3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('DebugFailCase3_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/catest",
						"name": "catest",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/catest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"\n",
							"# Designed to fail with a runtime error based on the input data\n",
							"from pyspark.sql.functions import udf\n",
							"from pyspark.sql.types import IntegerType\n",
							"\n",
							"def f(value):\n",
							"    return 100 / (value-74)\n",
							"udf_f = udf(f, IntegerType())\n",
							"\n",
							"u = \"abfss://datasets@contosolake.dfs.core.windows.net/SearchLog/SearchLog.parquet\"\n",
							"df = spark.read.load(u,format='parquet')\n",
							"df2 = df.withColumn( 'f', udf_f(\"latency\"))\n",
							"df2.show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"var a=100\n",
							"var b=0\n",
							"println(a/b)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"a=100\r\n",
							"b=0\r\n",
							"print(a/b)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugFailCase3_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('DebugFailCase3_Copy1_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/catest",
						"name": "catest",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/catest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"\n",
							"# Designed to fail with a runtime error based on the input data\n",
							"from pyspark.sql.functions import udf\n",
							"from pyspark.sql.types import IntegerType\n",
							"\n",
							"def f(value):\n",
							"    return 100 / (value-74)\n",
							"udf_f = udf(f, IntegerType())\n",
							"\n",
							"u = \"abfss://datasets@contosolake.dfs.core.windows.net/SearchLog/SearchLog.parquet\"\n",
							"df = spark.read.load(u,format='parquet')\n",
							"df2 = df.withColumn( 'f', udf_f(\"latency\"))\n",
							"df2.show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"var a=100\n",
							"var b=0\n",
							"println(a/b)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"a=100\r\n",
							"b=0\r\n",
							"print(a/b)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugFailCase5')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('DebugFailCase5_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/catest",
						"name": "catest",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/catest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# file already exist\n",
							"df = spark.createDataFrame([(1,1)],['Col_1','Col_2'])\n",
							"df.write.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/ouput.csv\",header = True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"var df = spark.read.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/ouput.csv\")\n",
							"df.write.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/ouput.csv\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugFailCase5_Copy2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('DebugFailCase5_Copy2_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/catest",
						"name": "catest",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/catest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# file already exist\n",
							"df = spark.createDataFrame([(1,1)],['Col_1','Col_2'])\n",
							"df.write.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/ouput.csv\",header = True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"var df = spark.read.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/ouput.csv\")\n",
							"df.write.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/ouput.csv\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugFailCase5_Copy3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('DebugFailCase5_Copy3_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/catest",
						"name": "catest",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/catest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# file already exist\n",
							"df = spark.createDataFrame([(1,1)],['Col_1','Col_2'])\n",
							"df.write.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/ouput.csv\",header = True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"var df = spark.read.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/ouput.csv\")\n",
							"df.write.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/ouput.csv\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugFailCase5_Copy4')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('DebugFailCase5_Copy4_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/catest",
						"name": "catest",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/catest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# file already exist\n",
							"df = spark.createDataFrame([(1,1)],['Col_1','Col_2'])\n",
							"df.write.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/ouput.csv\",header = True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"var df = spark.read.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/ouput.csv\")\n",
							"df.write.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/ouput.csv\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugFailCase6')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('DebugFailCase6_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/catest",
						"name": "catest",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/catest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# types\n",
							"a=1\n",
							"b=\"test\"\n",
							"print(a+b)   \n",
							"  "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"var a=100\n",
							"var b=\"This is a string\"\n",
							"println(a*b)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugFailCase7')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('DebugFailCase7_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# parameter not defined\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"var a=100\n",
							"println(a*b)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugFailCase8')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('DebugFailCase8_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/catest",
						"name": "catest",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/catest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# duplicate column header\n",
							"df = spark.createDataFrame([(1,1)],['Col_1','Col_1'])\n",
							"df.write.csv(\"abfss://datasets@contosolake.dfs.core.windows.net/SearchLog/dupheader.csv\",header = True,mode='overwrite')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"case class Person(name: String, name: String)\n",
							"var caseClassDS = Seq(Person(\"Andy\",32)).toDS()\n",
							"caseClassDS.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# duplicate column header\r\n",
							"df = spark.createDataFrame([(1,1)],['Col_1','Col_1'])\r\n",
							"df.write.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/cccouput.csv\",header = True,mode='overwrite')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugFailCase9')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('DebugFailCase9_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/catest",
						"name": "catest",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/catest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# the queries from raw JSON/CSV files are disallowed when the referenced columns only include the internal corrupt record column\n",
							"df = spark.createDataFrame([(1,1,1,1,1,1,1)],['Col_1','Col_2'])\n",
							"df.write.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/cccouput.csv\",header = False,mode='overwrite')\n",
							"df.show()\n",
							"dfread = spark.read.json(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/cccouput.csv\")\n",
							"dfread.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"case class Person(name: String, age: Long)\n",
							"var caseClassDS = Seq(Person(\"Andy\", 32, 45, \"extraColumn\")).toDS()\n",
							"caseClassDS.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugFailCases4')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('DebugFailCases4_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"u = \"abfss://datasets@contosolake.dfs.core.windows.net/SearchLog/SearchLog.parquet\"\n",
							"df = spark.read.load(u,format='parquet')\n",
							"df.show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"u = \"abfss://datasets@contosolake.dfs.core.windows.net/SearchLog/SearchLog1.parquet\"\n",
							"df = spark.read.load(u,format='parquet')\n",
							"df.show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"#tester side\n",
							"df=spark.read.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/cccccouput.csv/\",header=True)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"import spark.implicits._\n",
							"val peopleCSV = spark.read.csv(\"abfss://testzhao@hozhaogen2.dfs.core.windows.net/test/cccccouput.csv/\")\n",
							"peopleCSV.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getting Started with Delta Lake')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('Getting Started with Delta Lake_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (Python)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000000)\n",
							"delta_table_path = \"/delta/delta-table-{0}\".format(session_id)\n",
							"\n",
							"delta_table_path"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(0,5)\n",
							"data.show()\n",
							"data.write.format(\"delta\").save(delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").load(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(5,10)\n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Write data to a new managed catalog table.\n",
							"data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.sql(\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{0}'\".format(delta_table_path))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# List the 2 new tables.\n",
							"spark.sql(\"SHOW TABLES\").show()\n",
							"\n",
							"# Explore their properties.\n",
							"spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").show(truncate=False)\n",
							"spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").show(truncate=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"delta_table = DeltaTable.forPath(spark, delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Update every even value by adding 100 to it\n",
							"delta_table.update(\n",
							"  condition = expr(\"id % 2 == 0\"),\n",
							"  set = { \"id\": expr(\"id + 100\") })\n",
							"delta_table.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delete every even value\n",
							"delta_table.delete(\"id % 2 == 0\")\n",
							"delta_table.toDF().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Upsert (merge) new data\n",
							"new_data = spark.range(0,20).alias(\"newData\")\n",
							"\n",
							"delta_table.alias(\"oldData\")\\\n",
							"    .merge(new_data.alias(\"newData\"), \"oldData.id = newData.id\")\\\n",
							"    .whenMatchedUpdate(set = { \"id\": lit(\"-1\")})\\\n",
							"    .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") })\\\n",
							"    .execute()\n",
							"\n",
							"delta_table.toDF().show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().show(20, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"streaming_df = spark.readStream.format(\"rate\").load()\n",
							"stream = streaming_df\\\n",
							"    .selectExpr(\"value as id\")\\\n",
							"    .writeStream\\\n",
							"    .format(\"delta\")\\\n",
							"    .option(\"checkpointLocation\", \"/tmp/checkpoint-{0}\".format(session_id))\\\n",
							"    .start(delta_table_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.toDF().sort(col(\"id\").desc()).show(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"stream.stop()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(100, 1000, False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = \"/parquet/parquet-table-{0}\".format(session_id)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.convertToDelta(spark, \"parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"# Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DESCRIBE HISTORY delta.`{0}`\".format(delta_table_path)).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"VACUUM delta.`{0}`\".format(delta_table_path)).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_id = random.randint(0,1000)\n",
							"parquet_path = \"/parquet/parquet-table-{0}-{1}\".format(session_id, parquet_path)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)\n",
							"\n",
							"# Use SQL to convert the parquet table to Delta\n",
							"spark.sql(\"CONVERT TO DELTA parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/InputOutputTest')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('InputOutputTest_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"val rdd = sc.textFile(\"abfss://mydefault@ltianscusgen2.dfs.core.windows.net/xiaolel/test1/part-00004-c914640a-2e14-4687-9f5e-457ba4ebff8f-c000.csv\")\n",
							"rdd.count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"rdd.saveAsTextFile(\"/test/output\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"val another = spark.read.csv(\"abfss://mydefault@ltianscusgen2.dfs.core.windows.net/xiaolel/test1/part-00003-c914640a-2e14-4687-9f5e-457ba4ebff8f-c000.csv\")\n",
							"another.count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"another.write.csv(\"/dataframeoutput/test1\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/InputOutputTest_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('InputOutputTest_Copy1_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"val rdd = sc.textFile(\"abfss://mydefault@ltianscusgen2.dfs.core.windows.net/xiaolel/test1/part-00004-c914640a-2e14-4687-9f5e-457ba4ebff8f-c000.csv\")\n",
							"rdd.count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"rdd.saveAsTextFile(\"/test/output\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"val another = spark.read.csv(\"abfss://mydefault@ltianscusgen2.dfs.core.windows.net/xiaolel/test1/part-00003-c914640a-2e14-4687-9f5e-457ba4ebff8f-c000.csv\")\n",
							"another.count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"another.write.csv(\"/dataframeoutput/test1\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"hello world"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"test1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "test1/test2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"test in folder\r\n",
							"\r\n",
							"edit"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 4')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "test1/test2/test3"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"test in nested folder"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 5')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 6')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 7')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "test1/test2/test3"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"add new in nested folder"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 8')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"test 1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 9')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "test1/test2/test3"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"test in nested folder"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Synapse Notebook2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('Synapse Notebook2_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Charting in Synapse Notebook\n",
							"\n",
							"Synapse has common used data visualization packages pre installed, such as **matplotlib**, **bokeh**, **seaborn**, **altair**, **plotly**. This notebook provides examples to do data visualization using charts in Synapse notebook. \n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Matplotlib\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Line charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							" \n",
							"x  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
							"y1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\n",
							"y2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\n",
							"plt.plot(x, y1, label=\"line L\")\n",
							"plt.plot(x, y2, label=\"line H\")\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"x axis\")\n",
							"plt.ylabel(\"y axis\")\n",
							"plt.title(\"Line Graph Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# Bar chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"# Look at index 4 and 6, which demonstrate overlapping cases.\n",
							"x1 = [1, 3, 4, 5, 6, 7, 9]\n",
							"y1 = [4, 7, 2, 4, 7, 8, 3]\n",
							"\n",
							"x2 = [2, 4, 6, 8, 10]\n",
							"y2 = [5, 6, 2, 6, 2]\n",
							"\n",
							"# Colors: https://matplotlib.org/api/colors_api.html\n",
							"\n",
							"plt.bar(x1, y1, label=\"Blue Bar\", color='b')\n",
							"plt.bar(x2, y2, label=\"Green Bar\", color='g')\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"bar number\")\n",
							"plt.ylabel(\"bar height\")\n",
							"plt.title(\"Bar Chart Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Histogram\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Use numpy to generate a bunch of random data in a bell curve around 5.\n",
							"n = 5 + np.random.randn(1000)\n",
							"\n",
							"m = [m for m in range(len(n))]\n",
							"plt.bar(m, n)\n",
							"plt.title(\"Raw Data\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, bins=20)\n",
							"plt.title(\"Histogram\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, cumulative=True, bins=20)\n",
							"plt.title(\"Cumulative Histogram\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Scatter chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"x1 = [2, 3, 4]\n",
							"y1 = [5, 5, 5]\n",
							"\n",
							"x2 = [1, 2, 3, 4, 5]\n",
							"y2 = [2, 3, 2, 3, 4]\n",
							"y3 = [6, 8, 7, 8, 7]\n",
							"\n",
							"# Markers: https://matplotlib.org/api/markers_api.html\n",
							"\n",
							"plt.scatter(x1, y1)\n",
							"plt.scatter(x2, y2, marker='v', color='r')\n",
							"plt.scatter(x2, y3, marker='^', color='m')\n",
							"plt.title('Scatter Plot Example')\n",
							"plt.show()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Stack plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"idxes = [ 1,  2,  3,  4,  5,  6,  7,  8,  9]\n",
							"arr1  = [23, 40, 28, 43,  8, 44, 43, 18, 17]\n",
							"arr2  = [17, 30, 22, 14, 17, 17, 29, 22, 30]\n",
							"arr3  = [15, 31, 18, 22, 18, 19, 13, 32, 39]\n",
							"\n",
							"# Adding legend for stack plots is tricky.\n",
							"plt.plot([], [], color='r', label = 'D 1')\n",
							"plt.plot([], [], color='g', label = 'D 2')\n",
							"plt.plot([], [], color='b', label = 'D 3')\n",
							"\n",
							"plt.stackplot(idxes, arr1, arr2, arr3, colors= ['r', 'g', 'b'])\n",
							"plt.title('Stack Plot Example')\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Pie charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"labels = 'S1', 'S2', 'S3'\n",
							"sections = [56, 66, 24]\n",
							"colors = ['c', 'g', 'y']\n",
							"\n",
							"plt.pie(sections, labels=labels, colors=colors,\n",
							"        startangle=90,\n",
							"        explode = (0, 0.1, 0),\n",
							"        autopct = '%1.2f%%')\n",
							"\n",
							"plt.axis('equal') # Try commenting this out.\n",
							"plt.title('Pie Chart Example')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# fill_between and alpha\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"ys = 200 + np.random.randn(100)\n",
							"x = [x for x in range(len(ys))]\n",
							"\n",
							"plt.plot(x, ys, '-')\n",
							"plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)\n",
							"\n",
							"plt.title(\"Fills and Alpha Example\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Subplotting using Subplot2grid\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"def random_plots():\n",
							"  xs = []\n",
							"  ys = []\n",
							"  \n",
							"  for i in range(20):\n",
							"    x = i\n",
							"    y = np.random.randint(10)\n",
							"    \n",
							"    xs.append(x)\n",
							"    ys.append(y)\n",
							"  \n",
							"  return xs, ys\n",
							"\n",
							"fig = plt.figure()\n",
							"ax1 = plt.subplot2grid((5, 2), (0, 0), rowspan=1, colspan=2)\n",
							"ax2 = plt.subplot2grid((5, 2), (1, 0), rowspan=3, colspan=2)\n",
							"ax3 = plt.subplot2grid((5, 2), (4, 0), rowspan=1, colspan=1)\n",
							"ax4 = plt.subplot2grid((5, 2), (4, 1), rowspan=1, colspan=1)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax1.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax2.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax3.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax4.plot(x, y)\n",
							"\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# 3D Scatter Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"from mpl_toolkits.mplot3d import axes3d\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y1 = np.random.randint(10, size=10)\n",
							"z1 = np.random.randint(10, size=10)\n",
							"\n",
							"x2 = [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\n",
							"y2 = np.random.randint(-10, 0, size=10)\n",
							"z2 = np.random.randint(10, size=10)\n",
							"\n",
							"ax.scatter(x1, y1, z1, c='b', marker='o', label='blue')\n",
							"ax.scatter(x2, y2, z2, c='g', marker='D', label='green')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Scatter Plot Example\")\n",
							"plt.legend()\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# 3D Bar Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y = np.random.randint(10, size=10)\n",
							"z = np.zeros(10)\n",
							"\n",
							"dx = np.ones(10)\n",
							"dy = np.ones(10)\n",
							"dz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"\n",
							"ax.bar3d(x, y, z, dx, dy, dz, color='g')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Bar Chart Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Wireframe Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x, y, z = axes3d.get_test_data()\n",
							"\n",
							"ax.plot_wireframe(x, y, z, rstride = 2, cstride = 2)\n",
							"\n",
							"plt.title(\"Wireframe Plot Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Seaborn\n",
							"Seaborn is a library layered on top of Matplotlib that you can use."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Scatterplot with a nice regression line fit to it, all with just one call to Seaborn's regplot.\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"import seaborn as sns\n",
							"\n",
							"# Generate some random data\n",
							"num_points = 20\n",
							"# x will be 5, 6, 7... but also twiddled randomly\n",
							"x = 5 + np.arange(num_points) + np.random.randn(num_points)\n",
							"# y will be 10, 11, 12... but twiddled even more randomly\n",
							"y = 10 + np.arange(num_points) + 5 * np.random.randn(num_points)\n",
							"sns.regplot(x, y)\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Seanborn heatmap\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Make a 10 x 10 heatmap of some random data\n",
							"side_length = 10\n",
							"# Start with a 10 x 10 matrix with values randomized around 5\n",
							"data = 5 + np.random.randn(side_length, side_length)\n",
							"# The next two lines make the values larger as we get closer to (9, 9)\n",
							"data += np.arange(side_length)\n",
							"data += np.reshape(np.arange(side_length), (side_length, 1))\n",
							"# Generate the heatmap\n",
							"sns.heatmap(data)\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Bokeh\n",
							"You can render HTML or interactive libraries, like **bokeh**, using the **displayHTML()**.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import numpy as np\n",
							"from bokeh.plotting import figure, show\n",
							"from bokeh.io import output_notebook\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"\n",
							"N = 4000\n",
							"x = np.random.random(size=N) * 100\n",
							"y = np.random.random(size=N) * 100\n",
							"radii = np.random.random(size=N) * 1.5\n",
							"colors = [\"#%02x%02x%02x\" % (r, g, 150) for r, g in zip(np.floor(50+2*x).astype(int), np.floor(30+2*y).astype(int))]\n",
							"\n",
							"p = figure()\n",
							"p.circle(x, y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None)\n",
							"show(p)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Plotting glyphs over a map using bokeh.\n",
							"\n",
							"from bokeh.plotting import figure, output_file\n",
							"from bokeh.tile_providers import get_provider, Vendors\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"from bokeh.models import ColumnDataSource\n",
							"\n",
							"tile_provider = get_provider(Vendors.CARTODBPOSITRON)\n",
							"\n",
							"# range bounds supplied in web mercator coordinates\n",
							"p = figure(x_range=(-9000000,-8000000), y_range=(4000000,5000000),\n",
							"           x_axis_type=\"mercator\", y_axis_type=\"mercator\")\n",
							"p.add_tile(tile_provider)\n",
							"\n",
							"# plot datapoints on the map\n",
							"source = ColumnDataSource(\n",
							"    data=dict(x=[ -8800000, -8500000 , -8800000],\n",
							"              y=[4200000, 4500000, 4900000])\n",
							")\n",
							"\n",
							"p.circle(x=\"x\", y=\"y\", size=15, fill_color=\"blue\", fill_alpha=0.8, source=source)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/displayhtmlnotebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('displayhtmlnotebook_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import displayHTML\n",
							"displayHTML(\"hello world\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/displayhtmlnotebook_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('displayhtmlnotebook_Copy1_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import displayHTML\n",
							"displayHTML(\"hello world\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/displaynotebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('displaynotebook_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import display\n",
							"display.config(\"\")\n",
							"display(spark.range(1))\n",
							"display.execute(\"\") "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/matplotlibnotebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('matplotlibnotebook_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import matplotlib\n",
							"import matplotlib.pyplot as plt\n",
							"from notebookutils import enableMatplotlib\n",
							"from notebookutils.visualization.msInlinePlotlib import MsInLineBackend\n",
							"enableMatplotlib()\n",
							"if not matplotlib.__version__.startswith(\"3.3\") and (plt._show != MsInLineBackend.show):\n",
							"    plt._show = MsInLineBackend.show\n",
							"else:\n",
							"    pass\n",
							"plt.plot([1, 2, 3, 4])\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/matplotlibnotebook_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder 1"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('matplotlibnotebook_Copy1_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import matplotlib\n",
							"import matplotlib.pyplot as plt\n",
							"from notebookutils import enableMatplotlib\n",
							"from notebookutils.visualization.msInlinePlotlib import MsInLineBackend\n",
							"enableMatplotlib()\n",
							"if not matplotlib.__version__.startswith(\"3.3\") and (plt._show != MsInLineBackend.show):\n",
							"    plt._show = MsInLineBackend.show\n",
							"else:\n",
							"    pass\n",
							"plt.plot([1, 2, 3, 4])\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mssparkutilsnotebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('mssparkutilsnotebook_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils\n",
							"mssparkutils.fs.help()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.env.help()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "csharp"
							}
						},
						"source": [
							"%%csharp\n",
							"using static Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.Visualization.Functions;\n",
							"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.MSSparkUtils;\n",
							"FS.Help()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "csharp"
							}
						},
						"source": [
							"%%csharp\n",
							"Env.Help()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/outputAnalyzer')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder 1"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('outputAnalyzer_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## 0. Set the output location\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"//please replace the tagName in the path below for further analysis.\\nval baseOutputPath = \\\"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\\\"\\nval sparkConfPath = baseOutputPath + \\\"confJson\\\"\\nval queryMetricsPath = baseOutputPath + \\\"query-metrics/\\\"\\nval sparkEventsPath = baseOutputPath + \\\"spark-events/\\\"\\nval systemMetricsSummary = baseOutputPath + \\\"/summary/system-metrics-summary/\\\"\","
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"//please replace the tagName in the path below for further analysis.\n",
							"val baseOutputPath = \"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\"\n",
							"val sparkConfPath = baseOutputPath + \"confJson\"\n",
							"val queryMetricsPath = baseOutputPath + \"query-metrics/\"\n",
							"val sparkEventsPath = baseOutputPath + \"spark-events/\"\n",
							"val systemMetricsSummary = baseOutputPath + \"/summary/system-metrics-summary/\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 1. Spark Configurations\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkConf = spark.read.json(sparkConfPath)\\nsparkConf.select(\"spark.`spark.driver.cores`\", \"hadoop.`dfs.heartbeat.interval`\" ).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 2. queryMetrics: Get Query execution times\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Query(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    startTime: Long,\n",
							"    endTime:Long,\n",
							"    executionTimeMs: Long,\n",
							"    queryResultValidationSuccess: Boolean,\n",
							"    fileScanTimeMs: Long\n",
							"    )\n",
							"    \n",
							"    val queryMetrics = spark.read.option(\"basePath\", queryMetricsPath).json(s\"${queryMetricsPath}/*/**\")\n",
							"    val query = queryMetrics.select(\n",
							"        $\"queryId\",\n",
							"        $\"runId\",\n",
							"        $\"queryResultValidationSuccess\",\n",
							"        $\"runtimes.startTime\".alias(\"startTime\"),\n",
							"        $\"runTimes.endTime\".alias(\"endTime\"),\n",
							"        $\"runTimes.executionTimeMs\".alias(\"executionTimeMs\"),\n",
							"        $\"fileScanTimeMs\".alias(\"fileScanTimeMs\")\n",
							"        ).as[Query]\n",
							"        \n",
							"    queryMetrics.show(5)\n",
							"    query.createOrReplaceTempView(\"query\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"SELECT queryId, min(executionTimeMs), max(executionTimeMs) from query group by 1 order by 1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.0 Spark Events \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkEvents = spark.read.option(\"basePath\", sparkEventsPath).json(s\"${sparkEventsPath}/*/**\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.1 Spark Events: Task details\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Task(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    taskId: Long,\n",
							"    taskAttemptId: Long,\n",
							"    stageId: Long,\n",
							"    stageAttemptId: Long,\n",
							"    executorId: String,\n",
							"    host: String,\n",
							"    startTime: Long,\n",
							"    finishTime: Long,\n",
							"    killed: Boolean,\n",
							"    failed: Boolean,\n",
							"    shuffleBytesWritten: Long,\n",
							"    shuffleBytesRead: Long,\n",
							"    jvmGCTime: Long,\n",
							"    memoryBytesSpilled: Long,\n",
							"    diskBytesSpilled: Long,\n",
							"    taskType: String,\n",
							"    locality: String,\n",
							"    taskEndReason: String,\n",
							"    fileScanTime: Long)\n",
							"val taskEvents = sparkEvents.filter($\"Event\" === \"SparkListenerTaskEnd\").select(\n",
							"    $\"queryId\".alias(\"queryId\"),\n",
							"    $\"Task Info.Task ID\".alias(\"taskId\"),\n",
							"    $\"Task Info.Attempt\"alias(\"taskAttemptId\"),\n",
							"    $\"Stage ID\".alias(\"stageId\"),\n",
							"    $\"Stage Attempt ID\".alias(\"stageAttemptId\"),\n",
							"    $\"Task Info.Executor ID\".alias(\"executorId\"),\n",
							"    $\"Task Info.Host\".alias(\"host\"),\n",
							"    $\"Task Info.Launch Time\".alias(\"startTime\"),\n",
							"    $\"Task Info.Finish Time\".alias(\"finishTime\"),\n",
							"    $\"Task Info.Killed\".alias(\"killed\"),\n",
							"    $\"Task Info.Failed\".alias(\"failed\"),\n",
							"    $\"Task Metrics.Shuffle Write Metrics.Shuffle Bytes Written\".alias(\"shuffleBytesWritten\"),\n",
							"    ($\"Task Metrics.Shuffle Read Metrics.Remote Bytes Read\" + $\"Task Metrics.Shuffle Read Metrics.Local Bytes Read\").alias(\"shuffleBytesRead\\),\n",
							"    $\"Task Metrics.JVM GC Time\".alias(\"jvmGCTime\"),\n",
							"    $\"Task Metrics.Memory Bytes Spilled\".alias(\"memoryBytesSpilled\"),\n",
							"    $\"Task Metrics.Disk Bytes Spilled\".alias(\"diskBytesSpilled\") ,\n",
							"    $\"Task Type\".alias(\"taskType\"),\n",
							"    $\"Task Info.Locality\".alias(\"locality\"),\n",
							"    $\"Task End Reason\".alias(\"taskEndReason\"),\n",
							"    $\"Task Info.Accumulables.Value\".alias(\"accumulablesValue\"),\n",
							"    array_position($\"Task Info.Accumulables.Name\", \"scan time total (min, med, max)\").alias(\"fileScanTimeIndex\"))\n",
							"val task = taskEvents.select(\n",
							"    $\"queryId\",\n",
							"    $\"taskId\",\n",
							"    $\"taskAttemptId\",\n",
							"    $\"stageId\",\n",
							"    $\"stageAttemptId\",\n",
							"    $\"executorId\",\n",
							"    $\"host\",\n",
							"    $\"startTime\",\n",
							"    $\"finishTime\",\n",
							"    $\"killed\",\n",
							"    $\"failed\",\n",
							"    $\"shuffleBytesWritten\",\n",
							"    $\"shuffleBytesRead\",\n",
							"    $\"jvmGCTime\",\n",
							"    $\"memoryBytesSpilled\",\n",
							"    $\"diskBytesSpilled\",\n",
							"    $\"taskType\",\n",
							"    $\"locality\",\n",
							"    $\"taskEndReason\",\n",
							"    expr(\"CAST( Case when fileScanTimeIndex = 0 then 0 else element_at(accumulablesValue, CAST(fileScanTimeIndex AS INTEGER)) end AS BIGINT)\").alias(\"fileScanTime\")\n",
							"    ).as(\"t\").join(query.as(\"q\"), query(\"queryId\") === taskEvents(\"queryId\") &&  query(\"startTime\") < taskEvents(\"startTime\") && query(\"endTime\") > taskEvents(\"finishTime\")).select(\"t.*\", \"q.runId\").as[Task]\n",
							"task.createOrReplaceTempView(\"task\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"with totalTasks as\n",
							"(select queryId, runId, count(1) as totalTasks from task group by 1,2 order by 1,2),\n",
							"successfulTasks as\n",
							"(select queryId, runId, count(1) as successfulTasks from task where killed = \"false\" and failed = \"false\" group by 1,2 order by 1,2)\n",
							"select tt.queryId, tt.runId, totalTasks-successfulTasks as failedTasks, successfulTasks from totalTasks tt,successfulTasks st where st.queryId = tt.queryId and st.runId = tt.runId"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/outputAnalyzer_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder 1"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('outputAnalyzer_Copy1_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## 0. Set the output location\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"//please replace the tagName in the path below for further analysis.\\nval baseOutputPath = \\\"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\\\"\\nval sparkConfPath = baseOutputPath + \\\"confJson\\\"\\nval queryMetricsPath = baseOutputPath + \\\"query-metrics/\\\"\\nval sparkEventsPath = baseOutputPath + \\\"spark-events/\\\"\\nval systemMetricsSummary = baseOutputPath + \\\"/summary/system-metrics-summary/\\\"\","
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"//please replace the tagName in the path below for further analysis.\n",
							"val baseOutputPath = \"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\"\n",
							"val sparkConfPath = baseOutputPath + \"confJson\"\n",
							"val queryMetricsPath = baseOutputPath + \"query-metrics/\"\n",
							"val sparkEventsPath = baseOutputPath + \"spark-events/\"\n",
							"val systemMetricsSummary = baseOutputPath + \"/summary/system-metrics-summary/\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 1. Spark Configurations\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkConf = spark.read.json(sparkConfPath)\\nsparkConf.select(\"spark.`spark.driver.cores`\", \"hadoop.`dfs.heartbeat.interval`\" ).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 2. queryMetrics: Get Query execution times\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Query(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    startTime: Long,\n",
							"    endTime:Long,\n",
							"    executionTimeMs: Long,\n",
							"    queryResultValidationSuccess: Boolean,\n",
							"    fileScanTimeMs: Long\n",
							"    )\n",
							"    \n",
							"    val queryMetrics = spark.read.option(\"basePath\", queryMetricsPath).json(s\"${queryMetricsPath}/*/**\")\n",
							"    val query = queryMetrics.select(\n",
							"        $\"queryId\",\n",
							"        $\"runId\",\n",
							"        $\"queryResultValidationSuccess\",\n",
							"        $\"runtimes.startTime\".alias(\"startTime\"),\n",
							"        $\"runTimes.endTime\".alias(\"endTime\"),\n",
							"        $\"runTimes.executionTimeMs\".alias(\"executionTimeMs\"),\n",
							"        $\"fileScanTimeMs\".alias(\"fileScanTimeMs\")\n",
							"        ).as[Query]\n",
							"        \n",
							"    queryMetrics.show(5)\n",
							"    query.createOrReplaceTempView(\"query\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"SELECT queryId, min(executionTimeMs), max(executionTimeMs) from query group by 1 order by 1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.0 Spark Events \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkEvents = spark.read.option(\"basePath\", sparkEventsPath).json(s\"${sparkEventsPath}/*/**\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.1 Spark Events: Task details\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Task(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    taskId: Long,\n",
							"    taskAttemptId: Long,\n",
							"    stageId: Long,\n",
							"    stageAttemptId: Long,\n",
							"    executorId: String,\n",
							"    host: String,\n",
							"    startTime: Long,\n",
							"    finishTime: Long,\n",
							"    killed: Boolean,\n",
							"    failed: Boolean,\n",
							"    shuffleBytesWritten: Long,\n",
							"    shuffleBytesRead: Long,\n",
							"    jvmGCTime: Long,\n",
							"    memoryBytesSpilled: Long,\n",
							"    diskBytesSpilled: Long,\n",
							"    taskType: String,\n",
							"    locality: String,\n",
							"    taskEndReason: String,\n",
							"    fileScanTime: Long)\n",
							"val taskEvents = sparkEvents.filter($\"Event\" === \"SparkListenerTaskEnd\").select(\n",
							"    $\"queryId\".alias(\"queryId\"),\n",
							"    $\"Task Info.Task ID\".alias(\"taskId\"),\n",
							"    $\"Task Info.Attempt\"alias(\"taskAttemptId\"),\n",
							"    $\"Stage ID\".alias(\"stageId\"),\n",
							"    $\"Stage Attempt ID\".alias(\"stageAttemptId\"),\n",
							"    $\"Task Info.Executor ID\".alias(\"executorId\"),\n",
							"    $\"Task Info.Host\".alias(\"host\"),\n",
							"    $\"Task Info.Launch Time\".alias(\"startTime\"),\n",
							"    $\"Task Info.Finish Time\".alias(\"finishTime\"),\n",
							"    $\"Task Info.Killed\".alias(\"killed\"),\n",
							"    $\"Task Info.Failed\".alias(\"failed\"),\n",
							"    $\"Task Metrics.Shuffle Write Metrics.Shuffle Bytes Written\".alias(\"shuffleBytesWritten\"),\n",
							"    ($\"Task Metrics.Shuffle Read Metrics.Remote Bytes Read\" + $\"Task Metrics.Shuffle Read Metrics.Local Bytes Read\").alias(\"shuffleBytesRead\\),\n",
							"    $\"Task Metrics.JVM GC Time\".alias(\"jvmGCTime\"),\n",
							"    $\"Task Metrics.Memory Bytes Spilled\".alias(\"memoryBytesSpilled\"),\n",
							"    $\"Task Metrics.Disk Bytes Spilled\".alias(\"diskBytesSpilled\") ,\n",
							"    $\"Task Type\".alias(\"taskType\"),\n",
							"    $\"Task Info.Locality\".alias(\"locality\"),\n",
							"    $\"Task End Reason\".alias(\"taskEndReason\"),\n",
							"    $\"Task Info.Accumulables.Value\".alias(\"accumulablesValue\"),\n",
							"    array_position($\"Task Info.Accumulables.Name\", \"scan time total (min, med, max)\").alias(\"fileScanTimeIndex\"))\n",
							"val task = taskEvents.select(\n",
							"    $\"queryId\",\n",
							"    $\"taskId\",\n",
							"    $\"taskAttemptId\",\n",
							"    $\"stageId\",\n",
							"    $\"stageAttemptId\",\n",
							"    $\"executorId\",\n",
							"    $\"host\",\n",
							"    $\"startTime\",\n",
							"    $\"finishTime\",\n",
							"    $\"killed\",\n",
							"    $\"failed\",\n",
							"    $\"shuffleBytesWritten\",\n",
							"    $\"shuffleBytesRead\",\n",
							"    $\"jvmGCTime\",\n",
							"    $\"memoryBytesSpilled\",\n",
							"    $\"diskBytesSpilled\",\n",
							"    $\"taskType\",\n",
							"    $\"locality\",\n",
							"    $\"taskEndReason\",\n",
							"    expr(\"CAST( Case when fileScanTimeIndex = 0 then 0 else element_at(accumulablesValue, CAST(fileScanTimeIndex AS INTEGER)) end AS BIGINT)\").alias(\"fileScanTime\")\n",
							"    ).as(\"t\").join(query.as(\"q\"), query(\"queryId\") === taskEvents(\"queryId\") &&  query(\"startTime\") < taskEvents(\"startTime\") && query(\"endTime\") > taskEvents(\"finishTime\")).select(\"t.*\", \"q.runId\").as[Task]\n",
							"task.createOrReplaceTempView(\"task\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"with totalTasks as\n",
							"(select queryId, runId, count(1) as totalTasks from task group by 1,2 order by 1,2),\n",
							"successfulTasks as\n",
							"(select queryId, runId, count(1) as successfulTasks from task where killed = \"false\" and failed = \"false\" group by 1,2 order by 1,2)\n",
							"select tt.queryId, tt.runId, totalTasks-successfulTasks as failedTasks, successfulTasks from totalTasks tt,successfulTasks st where st.queryId = tt.queryId and st.runId = tt.runId"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/outputAnalyzer_Copy2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder 1"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('outputAnalyzer_Copy2_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## 0. Set the output location\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"//please replace the tagName in the path below for further analysis.\\nval baseOutputPath = \\\"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\\\"\\nval sparkConfPath = baseOutputPath + \\\"confJson\\\"\\nval queryMetricsPath = baseOutputPath + \\\"query-metrics/\\\"\\nval sparkEventsPath = baseOutputPath + \\\"spark-events/\\\"\\nval systemMetricsSummary = baseOutputPath + \\\"/summary/system-metrics-summary/\\\"\","
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"//please replace the tagName in the path below for further analysis.\n",
							"val baseOutputPath = \"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\"\n",
							"val sparkConfPath = baseOutputPath + \"confJson\"\n",
							"val queryMetricsPath = baseOutputPath + \"query-metrics/\"\n",
							"val sparkEventsPath = baseOutputPath + \"spark-events/\"\n",
							"val systemMetricsSummary = baseOutputPath + \"/summary/system-metrics-summary/\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 1. Spark Configurations\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkConf = spark.read.json(sparkConfPath)\\nsparkConf.select(\"spark.`spark.driver.cores`\", \"hadoop.`dfs.heartbeat.interval`\" ).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 2. queryMetrics: Get Query execution times\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Query(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    startTime: Long,\n",
							"    endTime:Long,\n",
							"    executionTimeMs: Long,\n",
							"    queryResultValidationSuccess: Boolean,\n",
							"    fileScanTimeMs: Long\n",
							"    )\n",
							"    \n",
							"    val queryMetrics = spark.read.option(\"basePath\", queryMetricsPath).json(s\"${queryMetricsPath}/*/**\")\n",
							"    val query = queryMetrics.select(\n",
							"        $\"queryId\",\n",
							"        $\"runId\",\n",
							"        $\"queryResultValidationSuccess\",\n",
							"        $\"runtimes.startTime\".alias(\"startTime\"),\n",
							"        $\"runTimes.endTime\".alias(\"endTime\"),\n",
							"        $\"runTimes.executionTimeMs\".alias(\"executionTimeMs\"),\n",
							"        $\"fileScanTimeMs\".alias(\"fileScanTimeMs\")\n",
							"        ).as[Query]\n",
							"        \n",
							"    queryMetrics.show(5)\n",
							"    query.createOrReplaceTempView(\"query\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"SELECT queryId, min(executionTimeMs), max(executionTimeMs) from query group by 1 order by 1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.0 Spark Events \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkEvents = spark.read.option(\"basePath\", sparkEventsPath).json(s\"${sparkEventsPath}/*/**\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.1 Spark Events: Task details\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Task(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    taskId: Long,\n",
							"    taskAttemptId: Long,\n",
							"    stageId: Long,\n",
							"    stageAttemptId: Long,\n",
							"    executorId: String,\n",
							"    host: String,\n",
							"    startTime: Long,\n",
							"    finishTime: Long,\n",
							"    killed: Boolean,\n",
							"    failed: Boolean,\n",
							"    shuffleBytesWritten: Long,\n",
							"    shuffleBytesRead: Long,\n",
							"    jvmGCTime: Long,\n",
							"    memoryBytesSpilled: Long,\n",
							"    diskBytesSpilled: Long,\n",
							"    taskType: String,\n",
							"    locality: String,\n",
							"    taskEndReason: String,\n",
							"    fileScanTime: Long)\n",
							"val taskEvents = sparkEvents.filter($\"Event\" === \"SparkListenerTaskEnd\").select(\n",
							"    $\"queryId\".alias(\"queryId\"),\n",
							"    $\"Task Info.Task ID\".alias(\"taskId\"),\n",
							"    $\"Task Info.Attempt\"alias(\"taskAttemptId\"),\n",
							"    $\"Stage ID\".alias(\"stageId\"),\n",
							"    $\"Stage Attempt ID\".alias(\"stageAttemptId\"),\n",
							"    $\"Task Info.Executor ID\".alias(\"executorId\"),\n",
							"    $\"Task Info.Host\".alias(\"host\"),\n",
							"    $\"Task Info.Launch Time\".alias(\"startTime\"),\n",
							"    $\"Task Info.Finish Time\".alias(\"finishTime\"),\n",
							"    $\"Task Info.Killed\".alias(\"killed\"),\n",
							"    $\"Task Info.Failed\".alias(\"failed\"),\n",
							"    $\"Task Metrics.Shuffle Write Metrics.Shuffle Bytes Written\".alias(\"shuffleBytesWritten\"),\n",
							"    ($\"Task Metrics.Shuffle Read Metrics.Remote Bytes Read\" + $\"Task Metrics.Shuffle Read Metrics.Local Bytes Read\").alias(\"shuffleBytesRead\\),\n",
							"    $\"Task Metrics.JVM GC Time\".alias(\"jvmGCTime\"),\n",
							"    $\"Task Metrics.Memory Bytes Spilled\".alias(\"memoryBytesSpilled\"),\n",
							"    $\"Task Metrics.Disk Bytes Spilled\".alias(\"diskBytesSpilled\") ,\n",
							"    $\"Task Type\".alias(\"taskType\"),\n",
							"    $\"Task Info.Locality\".alias(\"locality\"),\n",
							"    $\"Task End Reason\".alias(\"taskEndReason\"),\n",
							"    $\"Task Info.Accumulables.Value\".alias(\"accumulablesValue\"),\n",
							"    array_position($\"Task Info.Accumulables.Name\", \"scan time total (min, med, max)\").alias(\"fileScanTimeIndex\"))\n",
							"val task = taskEvents.select(\n",
							"    $\"queryId\",\n",
							"    $\"taskId\",\n",
							"    $\"taskAttemptId\",\n",
							"    $\"stageId\",\n",
							"    $\"stageAttemptId\",\n",
							"    $\"executorId\",\n",
							"    $\"host\",\n",
							"    $\"startTime\",\n",
							"    $\"finishTime\",\n",
							"    $\"killed\",\n",
							"    $\"failed\",\n",
							"    $\"shuffleBytesWritten\",\n",
							"    $\"shuffleBytesRead\",\n",
							"    $\"jvmGCTime\",\n",
							"    $\"memoryBytesSpilled\",\n",
							"    $\"diskBytesSpilled\",\n",
							"    $\"taskType\",\n",
							"    $\"locality\",\n",
							"    $\"taskEndReason\",\n",
							"    expr(\"CAST( Case when fileScanTimeIndex = 0 then 0 else element_at(accumulablesValue, CAST(fileScanTimeIndex AS INTEGER)) end AS BIGINT)\").alias(\"fileScanTime\")\n",
							"    ).as(\"t\").join(query.as(\"q\"), query(\"queryId\") === taskEvents(\"queryId\") &&  query(\"startTime\") < taskEvents(\"startTime\") && query(\"endTime\") > taskEvents(\"finishTime\")).select(\"t.*\", \"q.runId\").as[Task]\n",
							"task.createOrReplaceTempView(\"task\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"with totalTasks as\n",
							"(select queryId, runId, count(1) as totalTasks from task group by 1,2 order by 1,2),\n",
							"successfulTasks as\n",
							"(select queryId, runId, count(1) as successfulTasks from task where killed = \"false\" and failed = \"false\" group by 1,2 order by 1,2)\n",
							"select tt.queryId, tt.runId, totalTasks-successfulTasks as failedTasks, successfulTasks from totalTasks tt,successfulTasks st where st.queryId = tt.queryId and st.runId = tt.runId"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/outputAnalyzer_Copy3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder 1"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('outputAnalyzer_Copy3_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## 0. Set the output location\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"//please replace the tagName in the path below for further analysis.\\nval baseOutputPath = \\\"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\\\"\\nval sparkConfPath = baseOutputPath + \\\"confJson\\\"\\nval queryMetricsPath = baseOutputPath + \\\"query-metrics/\\\"\\nval sparkEventsPath = baseOutputPath + \\\"spark-events/\\\"\\nval systemMetricsSummary = baseOutputPath + \\\"/summary/system-metrics-summary/\\\"\","
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"//please replace the tagName in the path below for further analysis.\n",
							"val baseOutputPath = \"abfs://runresultstorage@tpcdsperfresults.dfs.core.windows.net/ayushifilescan1/sf_1000/\"\n",
							"val sparkConfPath = baseOutputPath + \"confJson\"\n",
							"val queryMetricsPath = baseOutputPath + \"query-metrics/\"\n",
							"val sparkEventsPath = baseOutputPath + \"spark-events/\"\n",
							"val systemMetricsSummary = baseOutputPath + \"/summary/system-metrics-summary/\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 1. Spark Configurations\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkConf = spark.read.json(sparkConfPath)\\nsparkConf.select(\"spark.`spark.driver.cores`\", \"hadoop.`dfs.heartbeat.interval`\" ).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 2. queryMetrics: Get Query execution times\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Query(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    startTime: Long,\n",
							"    endTime:Long,\n",
							"    executionTimeMs: Long,\n",
							"    queryResultValidationSuccess: Boolean,\n",
							"    fileScanTimeMs: Long\n",
							"    )\n",
							"    \n",
							"    val queryMetrics = spark.read.option(\"basePath\", queryMetricsPath).json(s\"${queryMetricsPath}/*/**\")\n",
							"    val query = queryMetrics.select(\n",
							"        $\"queryId\",\n",
							"        $\"runId\",\n",
							"        $\"queryResultValidationSuccess\",\n",
							"        $\"runtimes.startTime\".alias(\"startTime\"),\n",
							"        $\"runTimes.endTime\".alias(\"endTime\"),\n",
							"        $\"runTimes.executionTimeMs\".alias(\"executionTimeMs\"),\n",
							"        $\"fileScanTimeMs\".alias(\"fileScanTimeMs\")\n",
							"        ).as[Query]\n",
							"        \n",
							"    queryMetrics.show(5)\n",
							"    query.createOrReplaceTempView(\"query\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"SELECT queryId, min(executionTimeMs), max(executionTimeMs) from query group by 1 order by 1"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.0 Spark Events \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"val sparkEvents = spark.read.option(\"basePath\", sparkEventsPath).json(s\"${sparkEventsPath}/*/**\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## 3.1 Spark Events: Task details\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"case class Task(\n",
							"    queryId: String,\n",
							"    runId: Long,\n",
							"    taskId: Long,\n",
							"    taskAttemptId: Long,\n",
							"    stageId: Long,\n",
							"    stageAttemptId: Long,\n",
							"    executorId: String,\n",
							"    host: String,\n",
							"    startTime: Long,\n",
							"    finishTime: Long,\n",
							"    killed: Boolean,\n",
							"    failed: Boolean,\n",
							"    shuffleBytesWritten: Long,\n",
							"    shuffleBytesRead: Long,\n",
							"    jvmGCTime: Long,\n",
							"    memoryBytesSpilled: Long,\n",
							"    diskBytesSpilled: Long,\n",
							"    taskType: String,\n",
							"    locality: String,\n",
							"    taskEndReason: String,\n",
							"    fileScanTime: Long)\n",
							"val taskEvents = sparkEvents.filter($\"Event\" === \"SparkListenerTaskEnd\").select(\n",
							"    $\"queryId\".alias(\"queryId\"),\n",
							"    $\"Task Info.Task ID\".alias(\"taskId\"),\n",
							"    $\"Task Info.Attempt\"alias(\"taskAttemptId\"),\n",
							"    $\"Stage ID\".alias(\"stageId\"),\n",
							"    $\"Stage Attempt ID\".alias(\"stageAttemptId\"),\n",
							"    $\"Task Info.Executor ID\".alias(\"executorId\"),\n",
							"    $\"Task Info.Host\".alias(\"host\"),\n",
							"    $\"Task Info.Launch Time\".alias(\"startTime\"),\n",
							"    $\"Task Info.Finish Time\".alias(\"finishTime\"),\n",
							"    $\"Task Info.Killed\".alias(\"killed\"),\n",
							"    $\"Task Info.Failed\".alias(\"failed\"),\n",
							"    $\"Task Metrics.Shuffle Write Metrics.Shuffle Bytes Written\".alias(\"shuffleBytesWritten\"),\n",
							"    ($\"Task Metrics.Shuffle Read Metrics.Remote Bytes Read\" + $\"Task Metrics.Shuffle Read Metrics.Local Bytes Read\").alias(\"shuffleBytesRead\\),\n",
							"    $\"Task Metrics.JVM GC Time\".alias(\"jvmGCTime\"),\n",
							"    $\"Task Metrics.Memory Bytes Spilled\".alias(\"memoryBytesSpilled\"),\n",
							"    $\"Task Metrics.Disk Bytes Spilled\".alias(\"diskBytesSpilled\") ,\n",
							"    $\"Task Type\".alias(\"taskType\"),\n",
							"    $\"Task Info.Locality\".alias(\"locality\"),\n",
							"    $\"Task End Reason\".alias(\"taskEndReason\"),\n",
							"    $\"Task Info.Accumulables.Value\".alias(\"accumulablesValue\"),\n",
							"    array_position($\"Task Info.Accumulables.Name\", \"scan time total (min, med, max)\").alias(\"fileScanTimeIndex\"))\n",
							"val task = taskEvents.select(\n",
							"    $\"queryId\",\n",
							"    $\"taskId\",\n",
							"    $\"taskAttemptId\",\n",
							"    $\"stageId\",\n",
							"    $\"stageAttemptId\",\n",
							"    $\"executorId\",\n",
							"    $\"host\",\n",
							"    $\"startTime\",\n",
							"    $\"finishTime\",\n",
							"    $\"killed\",\n",
							"    $\"failed\",\n",
							"    $\"shuffleBytesWritten\",\n",
							"    $\"shuffleBytesRead\",\n",
							"    $\"jvmGCTime\",\n",
							"    $\"memoryBytesSpilled\",\n",
							"    $\"diskBytesSpilled\",\n",
							"    $\"taskType\",\n",
							"    $\"locality\",\n",
							"    $\"taskEndReason\",\n",
							"    expr(\"CAST( Case when fileScanTimeIndex = 0 then 0 else element_at(accumulablesValue, CAST(fileScanTimeIndex AS INTEGER)) end AS BIGINT)\").alias(\"fileScanTime\")\n",
							"    ).as(\"t\").join(query.as(\"q\"), query(\"queryId\") === taskEvents(\"queryId\") &&  query(\"startTime\") < taskEvents(\"startTime\") && query(\"endTime\") > taskEvents(\"finishTime\")).select(\"t.*\", \"q.runId\").as[Task]\n",
							"task.createOrReplaceTempView(\"task\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%sql\n",
							"with totalTasks as\n",
							"(select queryId, runId, count(1) as totalTasks from task group by 1,2 order by 1,2),\n",
							"successfulTasks as\n",
							"(select queryId, runId, count(1) as successfulTasks from task where killed = \"false\" and failed = \"false\" group by 1,2 order by 1,2)\n",
							"select tt.queryId, tt.runId, totalTasks-successfulTasks as failedTasks, successfulTasks from totalTasks tt,successfulTasks st where st.queryId = tt.queryId and st.runId = tt.runId"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xiaolelTest')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('xiaolelTest_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"println(\"hello spark print!\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"throw new Exception(\"Hey Spark Exception\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"throw new Error(\"Hey Spark Error!\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"1/0"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"System.err.println(\"Hey Spark Err Println!\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"hello, python print\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"raise Exception(\"python exception!!\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"1/0"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"import sys\n",
							"sys.stderr.write(\"python stderr write\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"source": [
							"sys.env.toList.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"spark.conf.getAll.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"import scala.collection.JavaConverters._\n",
							"import scala.collection.immutable.Seq\n",
							"spark.sparkContext.hadoopConfiguration.asScala.foreach(elem => {\n",
							"    println(\"\" + elem.getKey() + \" \\t\" + elem.getValue())\n",
							"})"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.log4j.LogManager\n",
							"val log = LogManager.getRootLogger\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"log.info(\"test log in driver\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"import com.microsoft.spark.notebook.visualization.displayHTML\n",
							"\n",
							"displayHTML(\"hello world!\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"import com.microsoft.spark.notebook.visualization.displayHTML\n",
							"\n",
							"displayHTML(\"<h1>My First Heading</h1><p>My first paragraph.</p>\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\n",
							"val deltaTablePath = s\"/delta/delta-table-$sessionId\";\n",
							"val data = spark.range(0, 5)\n",
							"data.write.format(\"delta\").save(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"val another = spark.read.load(deltaTablePath)\n",
							"another.count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"another.write.csv(\"/xiaolel/test1\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"val rdd = sc.textFile(\"abfss://mydefault@ltianscusgen2.dfs.core.windows.net/xiaolel/test1/part-00004-c914640a-2e14-4687-9f5e-457ba4ebff8f-c000.csv\")\n",
							"rdd.count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"rdd.saveAsTextFile(\"/xiaolel/rddoutputtest\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xiaolelTest1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('xiaolelTest1_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"sys.env.toList.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.conf.getAll.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"import scala.collection.JavaConverters._\n",
							"import scala.collection.immutable.Seq\n",
							"spark.sparkContext.hadoopConfiguration.asScala.foreach(elem => {\n",
							"    println(\"\" + elem.getKey() + \" \\t\" + elem.getValue())\n",
							"})"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.log4j.LogManager\n",
							"val log = LogManager.getRootLogger\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"log.info(\"test log in driver\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\n",
							"val deltaTablePath = s\"/delta/delta-table-$sessionId\";\n",
							"val anotherpath = s\"wasbs://historyevent@pyis63h7gq2ieyy6lzovj6yk.blob.core.windows.net/test\";\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"data.write.format(\"delta\").save(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xiaolelTest1_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('xiaolelTest1_Copy1_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"sys.env.toList.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.conf.getAll.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"import scala.collection.JavaConverters._\n",
							"import scala.collection.immutable.Seq\n",
							"spark.sparkContext.hadoopConfiguration.asScala.foreach(elem => {\n",
							"    println(\"\" + elem.getKey() + \" \\t\" + elem.getValue())\n",
							"})"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.log4j.LogManager\n",
							"val log = LogManager.getRootLogger\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"log.info(\"test log in driver\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\n",
							"val deltaTablePath = s\"/delta/delta-table-$sessionId\";\n",
							"val anotherpath = s\"wasbs://historyevent@pyis63h7gq2ieyy6lzovj6yk.blob.core.windows.net/test\";\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"data.write.format(\"delta\").save(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xiaolelTest1_Copy1_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "New folder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "[parameters('xiaolelTest1_Copy1_Copy1_properties_bigDataPool_referenceName')]",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/dantestpara/bigDataPools/origin",
						"name": "origin",
						"type": "Spark",
						"endpoint": "https://dantestpara.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/origin",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"sys.env.toList.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.conf.getAll.foreach(println)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"import scala.collection.JavaConverters._\n",
							"import scala.collection.immutable.Seq\n",
							"spark.sparkContext.hadoopConfiguration.asScala.foreach(elem => {\n",
							"    println(\"\" + elem.getKey() + \" \\t\" + elem.getValue())\n",
							"})"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.log4j.LogManager\n",
							"val log = LogManager.getRootLogger\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"log.info(\"test log in driver\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\n",
							"val deltaTablePath = s\"/delta/delta-table-$sessionId\";\n",
							"val anotherpath = s\"wasbs://historyevent@pyis63h7gq2ieyy6lzovj6yk.blob.core.windows.net/test\";\n",
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"data.write.format(\"delta\").save(deltaTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05121622_Scala_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "2.4",
				"language": "scala",
				"jobProperties": {
					"name": "05121622_Scala_Copy1",
					"file": "abfss://sparkjob@hozhaogen2.dfs.core.windows.net/synapse/workspaces/bigdataqa1105ws/batchjobs/0303_lanjun_scala_upload/wordcount.jar",
					"className": "WordCount",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1"
					},
					"args": [
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/scala/wordcount/shakespeare.txt",
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/scala/wordcount/result"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05121622_Scala_Copy2')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "2.4",
				"language": "scala",
				"jobProperties": {
					"name": "05121622_Scala_Copy2",
					"file": "abfss://sparkjob@hozhaogen2.dfs.core.windows.net/synapse/workspaces/bigdataqa1105ws/batchjobs/0303_lanjun_scala_upload/wordcount.jar",
					"className": "WordCount",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1"
					},
					"args": [
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/scala/wordcount/shakespeare.txt",
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/scala/wordcount/result"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05121622_Scala_Copy3')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "2.4",
				"language": "scala",
				"jobProperties": {
					"name": "05121622_Scala_Copy3",
					"file": "abfss://sparkjob@hozhaogen2.dfs.core.windows.net/synapse/workspaces/bigdataqa1105ws/batchjobs/0303_lanjun_scala_upload/wordcount.jar",
					"className": "WordCount ",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1"
					},
					"args": [
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/scala/wordcount/shakespeare.txt",
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/scala/wordcount/result"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05121622_Scala_Copy4')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "2.4",
				"language": "scala",
				"jobProperties": {
					"name": "05121622_Scala_Copy4",
					"file": "abfss://sparkjob@hozhaogen2.dfs.core.windows.net/synapse/workspaces/bigdataqa1105ws/batchjobs/0303_lanjun_scala_upload/wordcount.jar",
					"className": "WordCount",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1"
					},
					"args": [
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/scala/wordcount/shakespeare.txt",
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/scala/wordcount/result"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05121622_Scala_Copy5')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "2.4",
				"language": "scala",
				"jobProperties": {
					"name": "05121622_Scala_Copy5",
					"file": "abfss://sparkjob@hozhaogen2.dfs.core.windows.net/synapse/workspaces/bigdataqa1105ws/batchjobs/0303_lanjun_scala_upload/wordcount.jar",
					"className": "WordCount",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1"
					},
					"args": [
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/scala/wordcount/shakespeare.txt",
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/scala/wordcount/result"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05121622_Scala_Copy6')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "2.4",
				"language": "scala",
				"jobProperties": {
					"name": "05121622_Scala_Copy6",
					"file": "abfss://sparkjob@hozhaogen2.dfs.core.windows.net/synapse/workspaces/bigdataqa1105ws/batchjobs/0303_lanjun_scala_upload/wordcount.jar",
					"className": "WordCount",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1"
					},
					"args": [
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/scala/wordcount/shakespeare.txt",
						"abfss://sparkjob@hozhaogen2.dfs.core.windows.net/scala/wordcount/result"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugCase4')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "2.4",
				"language": "python",
				"jobProperties": {
					"name": "DebugCase4",
					"file": "abfss://mydefault@ltianwestus2gen2.dfs.core.windows.net/synapse/workspaces/ltianwestus2/batchjobs/DebugCase4/DebugCase4.py",
					"conf": {
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.dynamicAllocation.enabled": "false"
					},
					"args": [],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugCase5')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "catest",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "2.4",
				"language": "python",
				"jobProperties": {
					"name": "DebugCase5",
					"file": "abfss://mydefault@ltianwestus2gen2.dfs.core.windows.net/synapse/workspaces/ltianwestus2/batchjobs/DebugCase5/DebugCase5.py",
					"conf": {
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.dynamicAllocation.enabled": "false"
					},
					"args": [],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DebugCase7')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "catest",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "2.4",
				"language": "python",
				"jobProperties": {
					"name": "DebugCase7",
					"file": "abfss://mydefault@ltianwestus2gen2.dfs.core.windows.net/synapse/workspaces/ltianwestus2/batchjobs/DebugCase7/DebugCase7.py",
					"conf": {
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.dynamicAllocation.enabled": "false"
					},
					"args": [],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xiaolel-pythontest')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "origin",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "2.4",
				"language": "python",
				"jobProperties": {
					"name": "xiaolel-pythontest",
					"file": "abfss://mydefault@ltianscusgen2.dfs.core.windows.net/synapse/workspaces/ltianscusworkspace/batchjobs/Spark%20job%20definition%201/testlog.py",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1"
					},
					"args": [],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xiaolel-scalatest')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "catest",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "2.4",
				"language": "scala",
				"jobProperties": {
					"name": "xiaolel-scalatest",
					"file": "abfss://mydefault@ltianscusgen2.dfs.core.windows.net/synapse/workspaces/ltianscusworkspace/batchjobs/Spark%20job%20definition%201/debugtest1.jar",
					"className": "synapse.WasbIOTest",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1"
					},
					"args": [],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pool1')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"restorePointInTime": "0001-01-01T00:00:00",
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pool2')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"restorePointInTime": "0001-01-01T00:00:00",
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks",
			"apiVersion": "2019-06-01-preview",
			"properties": {},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/synapse-ws-sql--testpewsdeploy')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/testpewsdeploy",
				"groupId": "sql",
				"fqdns": [
					"testpewsdeploy.6a389ab6-cdf0-41a2-9514-e76d76b45065.sql.azuresynapse.net"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/synapse-ws-sqlOnDemand--testpewsdeploy')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/Dansynapse/providers/Microsoft.Synapse/workspaces/testpewsdeploy",
				"groupId": "sqlOnDemand",
				"fqdns": [
					"testpewsdeploy-ondemand.6a389ab6-cdf0-41a2-9514-e76d76b45065.sql.azuresynapse.net"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		}
	]
}
